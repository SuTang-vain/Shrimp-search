"""
GLM4.5æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°é›†æˆçš„GLM4.5æ¨¡å‹è¿›è¡Œå¯¹è¯å’Œæ¨ç†
"""

import os
from dotenv import load_dotenv
from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def demo_glm4_5_usage():
    """æ¼”ç¤ºGLM4.5æ¨¡å‹ä½¿ç”¨"""
    print("ğŸ¤– GLM4.5æ¨¡å‹ä½¿ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–å¢å¼ºLLMæ¥å£
        llm = EnhancedLLMInterfaceV2()
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
        available = llm.list_available_models()
        print(f"\nğŸ“‹ å¯ç”¨æ¨¡å‹æ€»æ•°: {len(available['remote']) + len(available['local'])}")
        
        # æ£€æŸ¥GLM4.5å¯ç”¨æ€§
        if 'glm4.5' in available['remote']:
            glm_info = available['remote']['glm4.5']
            if glm_info['available']:
                print("âœ… GLM4.5æ¨¡å‹å¯ç”¨ï¼Œå¼€å§‹æµ‹è¯•...")
                
                # è®¾ç½®GLM4.5æ¨¡å‹
                success = llm.set_model('glm4.5')
                if success:
                    print("âœ… æˆåŠŸåˆ‡æ¢åˆ°GLM4.5æ¨¡å‹")
                    
                    # æµ‹è¯•å¯¹è¯
                    test_queries = [
                        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
                        "ç”¨Pythonå†™ä¸€ä¸ªç®€å•çš„æ’åºç®—æ³•"
                    ]
                    
                    for i, query in enumerate(test_queries, 1):
                        print(f"\nğŸ” æµ‹è¯• {i}: {query}")
                        print("-" * 40)
                        
                        try:
                            response = llm.generate_response(query)
                            print(f"ğŸ¤– GLM4.5å›å¤: {response[:200]}...")
                            print("âœ… æµ‹è¯•æˆåŠŸ")
                        except Exception as e:
                            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                
                else:
                    print("âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥")
            else:
                print("âš ï¸  GLM4.5æ¨¡å‹éœ€è¦APIå¯†é’¥éªŒè¯")
        else:
            print("âŒ GLM4.5æ¨¡å‹æœªæ‰¾åˆ°")
            
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def demo_model_comparison():
    """æ¼”ç¤ºå¤šæ¨¡å‹å¯¹æ¯”"""
    print("\nğŸ”„ å¤šæ¨¡å‹å¯¹æ¯”æ¼”ç¤º")
    print("=" * 50)
    
    try:
        llm = EnhancedLLMInterfaceV2()
        
        # æµ‹è¯•é—®é¢˜
        test_query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€è¦è§£é‡Š"
        
        # å¯ç”¨çš„è¿œç¨‹æ¨¡å‹
        models_to_test = ['qwen2.5-7b', 'glm4.5']
        
        for model in models_to_test:
            print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹: {model}")
            print("-" * 30)
            
            try:
                # åˆ‡æ¢æ¨¡å‹
                if llm.set_model(model):
                    response = llm.generate_response(test_query)
                    print(f"å›å¤: {response[:150]}...")
                    print("âœ… æµ‹è¯•å®Œæˆ")
                else:
                    print("âŒ æ¨¡å‹ä¸å¯ç”¨")
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ å¯¹æ¯”æ¼”ç¤ºå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GLM4.5é›†æˆåŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    glm_key = os.getenv('GLM_API_KEY')
    if not glm_key:
        print("âŒ æœªæ‰¾åˆ°GLM_API_KEYç¯å¢ƒå˜é‡")
        return
    
    print(f"âœ… GLM APIå¯†é’¥å·²é…ç½®: {glm_key[:20]}...")
    
    # è¿è¡Œæ¼”ç¤º
    demo_glm4_5_usage()
    demo_model_comparison()
    
    print("\nğŸ‰ GLM4.5é›†æˆæ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print("1. ç¡®ä¿.envæ–‡ä»¶ä¸­é…ç½®äº†GLM_API_KEY")
    print("2. ä½¿ç”¨llm.set_model('glm4.5')åˆ‡æ¢åˆ°GLM4.5æ¨¡å‹")
    print("3. ä½¿ç”¨llm.generate_response(query)è¿›è¡Œå¯¹è¯")
    print("4. æ”¯æŒä¸å…¶ä»–æ¨¡å‹(Qwenã€æœ¬åœ°Ollama)æ— ç¼åˆ‡æ¢")

if __name__ == "__main__":
    main()