"""
å¢å¼ºçš„äº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ v2.0
é›†æˆå¤šç«¯æ¨¡å‹é€‰æ‹©åŠŸèƒ½ï¼Œæ”¯æŒModelScopeè¿œç¨‹APIå’ŒOllamaæœ¬åœ°éƒ¨ç½²
"""

import os
import sys
import json
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# å¯¼å…¥V2å¢å¼ºæ¨¡å—
from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2, GenerationResult
from enhanced_user_interface_v2 import EnhancedUserInterfaceV2
from model_manager import ModelManager
from ollama_interface import OllamaInterface

# å¯¼å…¥åŸæœ‰å¢å¼ºæ¨¡å—
from enhanced_multimodal_processor import EnhancedMultimodalProcessor
from enhanced_web_research import EnhancedWebResearchSystem
from performance_monitor import PerformanceMonitor, PerformanceContext
from enhanced_document_manager import EnhancedDocumentManager, DocumentChunk

# åŸºç¡€ä¾èµ–
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# åµŒå…¥æ¨¡å‹
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    print("âš  sentence-transformersåº“æœªå®‰è£…")
    EMBEDDING_AVAILABLE = False

class EnhancedVectorRetrieverV2:
    """å¢å¼ºçš„å‘é‡æ£€ç´¢å™¨V2 - æ”¯æŒå¤šæ¨¡å‹åµŒå…¥"""
    
    def __init__(self, embedding_model, performance_monitor: Optional[PerformanceMonitor] = None):
        self.embedding_model = embedding_model
        self.performance_monitor = performance_monitor
        self.documents = []
        self.doc_metadata = []
        self.document_ids = []
        self.doc_embeddings = None
        print("å¢å¼ºå‘é‡æ£€ç´¢å™¨V2åˆå§‹åŒ–å®Œæˆ")
    
    def add_documents(self, texts: List[str], metadata: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨"""
        if not texts:
            return
        
        operation_name = "add_documents_v2"
        additional_data = {"document_count": len(texts)}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                self._add_documents_impl(texts, metadata)
        else:
            self._add_documents_impl(texts, metadata)
    
    def _add_documents_impl(self, texts: List[str], metadata: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£çš„å®ç°"""
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            new_ids = [f"doc_{len(self.documents) + i}" for i in range(len(texts))]
            
            # å­˜å‚¨æ–‡æ¡£
            self.documents.extend(texts)
            self.doc_metadata.extend(metadata)
            self.document_ids.extend(new_ids)
            
            # è®¡ç®—åµŒå…¥
            print(f"æ­£åœ¨è®¡ç®— {len(texts)} ä¸ªæ–‡æ¡£çš„åµŒå…¥å‘é‡...")
            new_embeddings = self.embedding_model.encode(texts)
            
            if self.doc_embeddings is None:
                self.doc_embeddings = new_embeddings
            else:
                self.doc_embeddings = np.vstack([self.doc_embeddings, new_embeddings])
            
            print(f"æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªæ–‡æ¡£åˆ°æ£€ç´¢å™¨")
            
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def query(self, query_text: str, top_k: int = 5, filter_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸å…³æ–‡æ¡£"""
        if not self.documents or self.doc_embeddings is None:
            print("æ£€ç´¢å™¨ä¸­æ²¡æœ‰æ–‡æ¡£")
            return []
        
        operation_name = "vector_query_v2"
        additional_data = {"query_length": len(query_text), "top_k": top_k, "filter_type": filter_type}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                return self._query_impl(query_text, top_k, filter_type)
        else:
            return self._query_impl(query_text, top_k, filter_type)
    
    def _query_impl(self, query_text: str, top_k: int, filter_type: Optional[str]) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å®ç°"""
        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query_text])
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
            
            # è·å–top_kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k * 2]
            
            results = []
            for idx in top_indices:
                if idx >= len(self.documents):
                    continue
                    
                doc_type = self.doc_metadata[idx].get('type', 'unknown')
                if filter_type and doc_type != filter_type:
                    continue
                
                results.append({
                    'text': self.documents[idx],
                    'similarity': float(similarities[idx]),
                    'metadata': self.doc_metadata[idx],
                    'id': self.document_ids[idx]
                })
                
                if len(results) >= top_k:
                    break
            
            print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
            
        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return []

class EnhancedRAGSystemV2:
    """å¢å¼ºçš„RAGç³»ç»ŸV2 - é›†æˆå¤šç«¯æ¨¡å‹é€‰æ‹©"""
    
    def __init__(self, api_key: Optional[str] = None, 
                 enable_performance_monitoring: bool = True,
                 cache_dir: str = "document_cache"):
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor(enable_performance_monitoring) if enable_performance_monitoring else None
        
        try:
            print("ğŸš€ åˆå§‹åŒ–å¢å¼ºRAGç³»ç»ŸV2...")
            
            # åˆå§‹åŒ–V2 LLMæ¥å£
            self.llm = EnhancedLLMInterfaceV2(api_key=api_key)
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if not EMBEDDING_AVAILABLE:
                raise ImportError("sentence-transformersä¸å¯ç”¨")
            
            self.embedding_model = SentenceTransformer('intfloat/e5-large-v2')
            print("åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.vector_retriever = EnhancedVectorRetrieverV2(
                self.embedding_model, 
                self.performance_monitor
            )
            
            # åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨
            self.document_manager = EnhancedDocumentManager(
                cache_dir=cache_dir,
                enable_cache=True,
                max_cache_size_mb=1000
            )
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨
            self.multimodal_processor = EnhancedMultimodalProcessor()
            
            # åˆå§‹åŒ–ç½‘é¡µç ”ç©¶ç³»ç»Ÿ
            self.web_research_system = EnhancedWebResearchSystem()
            
            # ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = False
            self.multimodal_content_store = {}
            self.current_sources = []
            self.document_chunks_store = {}
            
            print("å¢å¼ºRAGç³»ç»ŸV2åˆå§‹åŒ–å®Œæˆ!")
            
        except Exception as e:
            print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def setup_model(self) -> bool:
        """è®¾ç½®æ¨¡å‹"""
        try:
            available_models = self.llm.list_available_models()
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œå°è¯•è‡ªåŠ¨é€‰æ‹©
            if not any(models for models in available_models.values()):
                print("æ²¡æœ‰å¯ç”¨æ¨¡å‹")
                return False
            
            # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©ç•Œé¢
            ui = EnhancedUserInterfaceV2()
            selected_model = ui.get_model_selection(available_models)
            
            if not selected_model:
                print("æœªé€‰æ‹©æ¨¡å‹")
                return False
            
            if selected_model == 'auto':
                # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
                selected_model = self.llm.auto_select_best_model()
                if not selected_model:
                    print("è‡ªåŠ¨é€‰æ‹©æ¨¡å‹å¤±è´¥")
                    return False
            else:
                # æ‰‹åŠ¨é€‰æ‹©æ¨¡å‹
                if not self.llm.set_model(selected_model):
                    print(f"è®¾ç½®æ¨¡å‹å¤±è´¥: {selected_model}")
                    return False
            
            # æµ‹è¯•æ¨¡å‹
            print("æ­£åœ¨æµ‹è¯•æ¨¡å‹...")
            test_result = self.llm.test_model()
            ui.display_test_result(test_result)
            
            return test_result['success']
            
        except Exception as e:
            print(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def setup_knowledge_base(self, sources: List[str] = None, source_type: str = "path", 
                           force_reprocess: bool = False):
        """è®¾ç½®çŸ¥è¯†åº“ - æ”¯æŒå¤šæ–‡æ¡£å’Œå¢é‡æ›´æ–°"""
        if sources is None:
            sources = []
        
        operation_name = "setup_knowledge_base_v2"
        additional_data = {"source_type": source_type, "source_count": len(sources)}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
        else:
            self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
    
    def _setup_knowledge_base_impl(self, sources: List[str], source_type: str, 
                                 force_reprocess: bool):
        """çŸ¥è¯†åº“è®¾ç½®å®ç°"""
        try:
            print(f"æ­£åœ¨è®¾ç½®çŸ¥è¯†åº“...")
            print(f"æ–‡æ¡£æ•°é‡: {len(sources)}")
            print(f"æ¥æºç±»å‹: {source_type}")
            
            all_texts_to_embed = []
            all_metadata_to_embed = []
            total_chunks = 0
            processing_stats = {"text": 0, "image": 0, "table": 0, "other": 0}
            
            for source in sources:
                print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {source}")
                
                # ç¡®ä¿sourceæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¸æ˜¯åˆ—è¡¨
                if isinstance(source, list):
                    print(f"è­¦å‘Š: æºæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ : {source}")
                    if source:
                        source = source[0]
                    else:
                        print("è·³è¿‡ç©ºåˆ—è¡¨æº")
                        continue
                
                # éªŒè¯è·¯å¾„å­˜åœ¨
                if source_type == "path" and not os.path.exists(source):
                    print(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {source}")
                    continue
                
                try:
                    # ä½¿ç”¨æ–‡æ¡£ç®¡ç†å™¨å¤„ç†æ–‡æ¡£
                    if source_type == "path":
                        document_chunks = self.document_manager.process_document(
                            source, force_reprocess=force_reprocess
                        )
                    else:
                        # å¯¹äºURLï¼Œå…ˆä¸‹è½½å†å¤„ç†
                        document_chunks = self._process_url_document(source, force_reprocess)
                    
                    # å­˜å‚¨æ–‡æ¡£å—
                    self.document_chunks_store[source] = document_chunks
                    
                    # è½¬æ¢ä¸ºå‘é‡æ£€ç´¢æ ¼å¼
                    for chunk in document_chunks:
                        all_texts_to_embed.append(chunk.content)
                        
                        # å¢å¼ºå…ƒæ•°æ®
                        enhanced_metadata = chunk.metadata.copy()
                        enhanced_metadata.update({
                            'chunk_id': chunk.chunk_id,
                            'chunk_type': chunk.chunk_type,
                            'source_file': source
                        })
                        all_metadata_to_embed.append(enhanced_metadata)
                        
                        # ç»Ÿè®¡
                        chunk_type = chunk.chunk_type
                        if chunk_type in processing_stats:
                            processing_stats[chunk_type] += 1
                        else:
                            processing_stats["other"] += 1
                    
                    total_chunks += len(document_chunks)
                    print(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {len(document_chunks)} ä¸ªå—")
                    
                except Exception as e:
                    print(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
                    continue
            
            if not all_texts_to_embed:
                raise ValueError("æœªèƒ½ä»ä»»ä½•æ–‡æ¡£ä¸­æå–åˆ°å†…å®¹")
            
            # æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨
            print(f"\næ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
            self.vector_retriever.add_documents(all_texts_to_embed, all_metadata_to_embed)
            
            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = True
            self.current_sources = sources
            
            print(f"\nçŸ¥è¯†åº“è®¾ç½®å®Œæˆ!")
            print(f"å¤„ç†æ–‡æ¡£æ•°: {len(sources)}")
            print(f"æ€»æ–‡æ¡£å—æ•°: {total_chunks}")
            print(f"å†…å®¹ç»Ÿè®¡: æ–‡æœ¬({processing_stats['text']}) å›¾åƒ({processing_stats['image']}) è¡¨æ ¼({processing_stats['table']}) å…¶ä»–({processing_stats['other']})")
            
            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            cache_stats = self.document_manager.get_cache_stats()
            print(f"ç¼“å­˜ç»Ÿè®¡: {cache_stats['cached_documents']} ä¸ªæ–‡æ¡£, {cache_stats['total_size_mb']:.1f}MB")
            
        except Exception as e:
            print(f"çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _process_url_document(self, url: str, force_reprocess: bool) -> List[DocumentChunk]:
        """å¤„ç†URLæ–‡æ¡£"""
        import requests
        import tempfile
        from urllib.parse import urlparse
        
        try:
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            file_ext = os.path.splitext(parsed_url.path)[1]
            if not file_ext:
                file_ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=file_ext, delete=False) as temp_file:
                temp_file.write(response.content)
                temp_path = temp_file.name
            
            try:
                # å¤„ç†æ–‡æ¡£
                chunks = self.document_manager.process_document(temp_path, force_reprocess)
                
                # æ›´æ–°å…ƒæ•°æ®ä¸­çš„æºä¿¡æ¯
                for chunk in chunks:
                    chunk.metadata['original_url'] = url
                    chunk.metadata['source'] = url
                
                return chunks
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            print(f"URLæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return []
    
    def enhanced_query_v2(self, query: str, retrieval_mode: str = "å¿«é€Ÿæ£€ç´¢", 
                         generation_settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºæŸ¥è¯¢V2 - æ”¯æŒè‡ªå®šä¹‰ç”Ÿæˆè®¾ç½®"""
        if not self.knowledge_base_initialized:
            raise RuntimeError("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        if not self.llm.current_backend:
            raise RuntimeError("æœªè®¾ç½®æ¨¡å‹")
        
        operation_name = f"enhanced_query_v2_{retrieval_mode}"
        additional_data = {"query_length": len(query), "mode": retrieval_mode}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                return self._enhanced_query_v2_impl(query, retrieval_mode, generation_settings)
        else:
            return self._enhanced_query_v2_impl(query, retrieval_mode, generation_settings)
    
    def _enhanced_query_v2_impl(self, query: str, retrieval_mode: str, 
                               generation_settings: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢V2å®ç°"""
        try:
            print(f"\næ‰§è¡Œ{retrieval_mode}: {query}")
            print("="*60)
            
            # è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°
            gen_settings = generation_settings or {}
            max_tokens = gen_settings.get('max_tokens', 800)
            temperature = gen_settings.get('temperature', 0.7)
            
            if retrieval_mode == "å¿«é€Ÿæ£€ç´¢":
                return self._quick_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "æ·±åº¦æ£€ç´¢":
                return self._deep_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "ä¸»é¢˜æ£€ç´¢":
                return self._topic_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "æ™ºèƒ½æ£€ç´¢":
                return self._smart_retrieval_v2(query, max_tokens, temperature)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢æ¨¡å¼: {retrieval_mode}")
                
        except Exception as e:
            print(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}"}
    
    def _quick_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """å¿«é€Ÿæ£€ç´¢V2"""
        print("æ‰§è¡Œå¿«é€Ÿæ£€ç´¢...")
        
        # ç›´æ¥å‘é‡æ£€ç´¢
        retrieved_docs = self.vector_retriever.query(query, top_k=3, filter_type='text')
        
        if not retrieved_docs:
            return {"error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"}
        
        # ç”Ÿæˆç­”æ¡ˆ
        context_texts = [doc['text'] for doc in retrieved_docs]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç®€æ´æ˜ç¡®çš„ç­”æ¡ˆï¼š"""
        
        # ä½¿ç”¨V2æ¥å£ç”Ÿæˆ
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'retrieved_docs': retrieved_docs,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'å¿«é€Ÿæ£€ç´¢V2',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _deep_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """æ·±åº¦æ£€ç´¢V2"""
        print("æ‰§è¡Œæ·±åº¦æ£€ç´¢...")
        
        # æ­¥éª¤1: æŸ¥è¯¢é‡å†™
        print("æ­¥éª¤1: æŸ¥è¯¢é‡å†™")
        rewritten_query = self._query_rewriting_v2(query)
        
        # æ­¥éª¤2: HyDEç”Ÿæˆ
        print("æ­¥éª¤2: HyDEå‡è®¾æ–‡æ¡£ç”Ÿæˆ")
        hyde_doc = self._hyde_generation_v2(rewritten_query)
        
        # æ­¥éª¤3: å¤šè·¯æ£€ç´¢
        print("æ­¥éª¤3: å¤šè·¯æ£€ç´¢")
        all_results = []
        
        # åŸå§‹æŸ¥è¯¢æ£€ç´¢
        original_results = self.vector_retriever.query(query, top_k=5)
        all_results.append(original_results)
        
        # é‡å†™æŸ¥è¯¢æ£€ç´¢
        rewritten_results = self.vector_retriever.query(rewritten_query, top_k=5)
        all_results.append(rewritten_results)
        
        # HyDEæ–‡æ¡£æ£€ç´¢
        hyde_results = self.vector_retriever.query(hyde_doc, top_k=5)
        all_results.append(hyde_results)
        
        # å¤šæ¨¡æ€æ£€ç´¢
        image_results = self.vector_retriever.query(query, top_k=3, filter_type='image')
        if image_results:
            all_results.append(image_results)
        
        table_results = self.vector_retriever.query(query, top_k=3, filter_type='table')
        if table_results:
            all_results.append(table_results)
        
        # æ­¥éª¤4: RRFèåˆ
        print("æ­¥éª¤4: RRFç»“æœèåˆ")
        final_docs = self._rrf_fusion(all_results)
        
        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        print("æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        context_texts = [doc['text'] for doc in final_docs[:5]]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹
2. æä¾›å‡†ç¡®è¯¦ç»†çš„ç­”æ¡ˆ
3. å¼•ç”¨å…·ä½“æ–‡æ¡£å†…å®¹
4. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„

è¯·æä¾›è¯¦ç»†ç­”æ¡ˆï¼š"""
        
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'rewritten_query': rewritten_query,
            'hyde_doc': hyde_doc,
            'retrieved_docs': final_docs,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'æ·±åº¦æ£€ç´¢V2(é‡å†™+HyDE+RRF)',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _topic_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """ä¸»é¢˜æ£€ç´¢V2"""
        print("æ‰§è¡Œä¸»é¢˜æ£€ç´¢...")
        
        # æ­¥éª¤1: PDFæ£€ç´¢
        print("æ­¥éª¤1: PDFæ–‡æ¡£æ£€ç´¢")
        pdf_docs = self.vector_retriever.query(query, top_k=5)
        
        # æ­¥éª¤2: ç½‘é¡µç ”ç©¶
        print("æ­¥éª¤2: ç½‘é¡µç ”ç©¶")
        web_research_result = self.web_research_system.research_topic(query, max_pages=3)
        
        # æ­¥éª¤3: ç»¼åˆåˆ†æ
        print("æ­¥éª¤3: ç»¼åˆåˆ†æ")
        
        # åˆå¹¶å†…å®¹
        combined_context = ""
        
        # æ·»åŠ PDFå†…å®¹
        if pdf_docs:
            pdf_context = "\n".join([doc['text'] for doc in pdf_docs])
            combined_context += f"PDFæ–‡æ¡£å†…å®¹ï¼š\n{pdf_context}\n\n"
        
        # æ·»åŠ ç½‘é¡µå†…å®¹
        web_analysis = web_research_result.get('analysis', '')
        if web_analysis and not web_analysis.startswith("æœªèƒ½è·å–"):
            combined_context += f"ç½‘é¡µç ”ç©¶å†…å®¹ï¼š\n{web_analysis}\n\n"
        
        if not combined_context.strip():
            combined_context = f"å…³äº'{query}'çš„ä¿¡æ¯ä¸»è¦æ¥è‡ªPDFæ–‡æ¡£ï¼Œç½‘é¡µç ”ç©¶åŠŸèƒ½å½“å‰å—é™ã€‚"
        
        # ç”Ÿæˆç»¼åˆç­”æ¡ˆ
        prompt = f"""åŸºäºä»¥ä¸‹PDFæ–‡æ¡£å’Œç½‘é¡µç ”ç©¶ä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼š

{combined_context}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç»¼åˆæ€§çš„è¯¦ç»†ç­”æ¡ˆï¼Œæ•´åˆPDFå’Œç½‘é¡µä¿¡æ¯ï¼š"""
        
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'pdf_docs': pdf_docs,
            'web_research': web_research_result,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'ä¸»é¢˜æ£€ç´¢V2(PDF+ç½‘é¡µ)',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _smart_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€ç´¢V2 - è‡ªé€‚åº”é€‰æ‹©æœ€ä½³ç­–ç•¥"""
        print("æ‰§è¡Œæ™ºèƒ½æ£€ç´¢...")
        
        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
        query_complexity = self._analyze_query_complexity(query)
        
        print(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æ: {query_complexity}")
        
        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©ç­–ç•¥
        if query_complexity['complexity_score'] < 0.3:
            print("é€‰æ‹©å¿«é€Ÿæ£€ç´¢ç­–ç•¥")
            return self._quick_retrieval_v2(query, max_tokens, temperature)
        elif query_complexity['complexity_score'] < 0.7:
            print("é€‰æ‹©æ·±åº¦æ£€ç´¢ç­–ç•¥")
            return self._deep_retrieval_v2(query, max_tokens, temperature)
        else:
            print("é€‰æ‹©ä¸»é¢˜æ£€ç´¢ç­–ç•¥")
            return self._topic_retrieval_v2(query, max_tokens, temperature)
    
    def _analyze_query_complexity(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        complexity_indicators = {
            'length': len(query.split()),
            'has_multiple_questions': '?' in query and query.count('?') > 1,
            'has_comparison': any(word in query.lower() for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'vs', 'åŒºåˆ«', 'ä¸åŒ']),
            'has_analysis': any(word in query.lower() for word in ['åˆ†æ', 'è§£é‡Š', 'åŸå› ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•']),
            'has_synthesis': any(word in query.lower() for word in ['ç»¼åˆ', 'æ€»ç»“', 'æ¦‚æ‹¬', 'æ•´ä½“'])
        }
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        score = 0.0
        if complexity_indicators['length'] > 10:
            score += 0.2
        if complexity_indicators['has_multiple_questions']:
            score += 0.3
        if complexity_indicators['has_comparison']:
            score += 0.2
        if complexity_indicators['has_analysis']:
            score += 0.2
        if complexity_indicators['has_synthesis']:
            score += 0.3
        
        return {
            'complexity_score': min(score, 1.0),
            'indicators': complexity_indicators
        }
    
    def _query_rewriting_v2(self, query: str) -> str:
        """æŸ¥è¯¢é‡å†™V2"""
        try:
            prompt = f"""è¯·é‡å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ–‡æ¡£æ£€ç´¢ï¼š

åŸå§‹æŸ¥è¯¢ï¼š{query}

é‡å†™è¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒæ„å›¾
2. ä½¿ç”¨æ›´å…·ä½“çš„è¯æ±‡
3. é€‚åˆå‘é‡æ£€ç´¢
4. åªè¿”å›é‡å†™åçš„æŸ¥è¯¢

é‡å†™æŸ¥è¯¢ï¼š"""
            
            result = self.llm.generate(prompt, max_tokens=100, temperature=0.1)
            
            if result.error:
                print(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {result.error}")
                return query
            
            rewritten = result.content.strip()
            
            # æ¸…ç†ç»“æœ
            if rewritten and not rewritten.startswith("æŠ±æ­‰"):
                lines = rewritten.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('é‡å†™') and not line.startswith('åŸå§‹'):
                        return line
            
            return query
            
        except Exception as e:
            print(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {e}")
            return query
    
    def _hyde_generation_v2(self, query: str) -> str:
        """HyDEå‡è®¾æ–‡æ¡£ç”ŸæˆV2"""
        try:
            prompt = f"""åŸºäºæŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§æ–‡æ¡£ç‰‡æ®µï¼š

æŸ¥è¯¢ï¼š{query}

è¦æ±‚ï¼š
1. ç”Ÿæˆ200-300å­—çš„æ–‡æ¡£ç‰‡æ®µ
2. ç›´æ¥å›ç­”æŸ¥è¯¢é—®é¢˜
3. ä½¿ç”¨ä¸“ä¸šå‡†ç¡®çš„è¯­è¨€
4. åŒ…å«ç›¸å…³æŠ€æœ¯ç»†èŠ‚

å‡è®¾æ–‡æ¡£ï¼š"""
            
            result = self.llm.generate(prompt, max_tokens=300, temperature=0.3)
            
            if result.error:
                print(f"HyDEç”Ÿæˆå¤±è´¥: {result.error}")
                return query
            
            hyde_doc = result.content.strip()
            
            if hyde_doc and not hyde_doc.startswith("æŠ±æ­‰"):
                return hyde_doc
            else:
                return query
                
        except Exception as e:
            print(f"HyDEç”Ÿæˆå¤±è´¥: {e}")
            return query
    
    def _rrf_fusion(self, all_results: List[List[Dict]], k: int = 60) -> List[Dict[str, Any]]:
        """RRFèåˆå¤šä¸ªæ£€ç´¢ç»“æœ"""
        try:
            doc_scores = {}
            
            for results in all_results:
                for rank, doc in enumerate(results):
                    doc_id = doc.get('id', '')
                    if doc_id:
                        rrf_score = 1.0 / (k + rank + 1)
                        if doc_id in doc_scores:
                            doc_scores[doc_id]['score'] += rrf_score
                        else:
                            doc_scores[doc_id] = {
                                'doc': doc,
                                'score': rrf_score
                            }
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)
            
            # æ·»åŠ RRFåˆ†æ•°
            final_docs = []
            for item in sorted_docs:
                doc = item['doc'].copy()
                doc['rrf_score'] = item['score']
                final_docs.append(doc)
            
            return final_docs
            
        except Exception as e:
            print(f"RRFèåˆå¤±è´¥: {e}")
            return all_results[0] if all_results else []
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if self.performance_monitor:
            return self.performance_monitor.get_stats()
        return {}
    
    def get_model_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        return self.llm.generation_stats
    
    def switch_model(self, model_key: str) -> bool:
        """åˆ‡æ¢æ¨¡å‹"""
        return self.llm.set_model(model_key)
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return self.llm.list_available_models()
    
    def get_current_model_info(self) -> Optional[Dict[str, Any]]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return self.llm.get_current_model()

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç³»ç»ŸåŠŸèƒ½"""
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        api_key = os.getenv('MODELSCOPE_API_KEY')
        
        print("="*60)
        print("å¢å¼ºäº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ V2.0")
        print("æ”¯æŒå¤šç«¯æ¨¡å‹é€‰æ‹© (ModelScope + Ollama)")
        print("="*60)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system = EnhancedRAGSystemV2(
            api_key=api_key,
            enable_performance_monitoring=True
        )
        
        # è®¾ç½®æ¨¡å‹
        print("\næ­¥éª¤1: æ¨¡å‹è®¾ç½®")
        if not rag_system.setup_model():
            print("æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯
        current_model = rag_system.get_current_model_info()
        if current_model:
            print(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {current_model['key']} ({current_model['provider']})")
        
        # è®¾ç½®çŸ¥è¯†åº“
        print("\næ­¥éª¤2: çŸ¥è¯†åº“è®¾ç½®")
        ui = EnhancedUserInterfaceV2()
        
        # è·å–æ–‡æ¡£è·¯å¾„
        document_sources = ui.get_document_sources()
        if not document_sources:
            print("æœªæä¾›æ–‡æ¡£ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ¼”ç¤ºæ–‡æ¡£
            document_sources = []
        
        if document_sources:
            rag_system.setup_knowledge_base(
                sources=document_sources,
                source_type="path",
                force_reprocess=False
            )
        
        # äº¤äº’å¼æŸ¥è¯¢å¾ªç¯
        print("\næ­¥éª¤3: å¼€å§‹äº¤äº’å¼æŸ¥è¯¢")
        print("è¾“å…¥ 'quit' é€€å‡ºï¼Œ'stats' æŸ¥çœ‹ç»Ÿè®¡ï¼Œ'switch' åˆ‡æ¢æ¨¡å‹")
        print("-" * 60)
        
        while True:
            try:
                query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not query:
                    continue
                
                if query.lower() == 'quit':
                    break
                
                if query.lower() == 'stats':
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    perf_stats = rag_system.get_performance_stats()
                    model_stats = rag_system.get_model_stats()
                    
                    print("\næ€§èƒ½ç»Ÿè®¡:")
                    if perf_stats:
                        for operation, stats in perf_stats.items():
                            print(f"  {operation}: {stats.get('count', 0)} æ¬¡, å¹³å‡è€—æ—¶: {stats.get('avg_time', 0):.2f}s")
                    
                    print("\næ¨¡å‹ç»Ÿè®¡:")
                    print(f"  æ€»è¯·æ±‚æ•°: {model_stats.get('total_requests', 0)}")
                    print(f"  æˆåŠŸè¯·æ±‚: {model_stats.get('successful_requests', 0)}")
                    print(f"  å¤±è´¥è¯·æ±‚: {model_stats.get('failed_requests', 0)}")
                    print(f"  æ€»è€—æ—¶: {model_stats.get('total_time', 0):.2f}s")
                    
                    model_usage = model_stats.get('model_usage', {})
                    if model_usage:
                        print("  æ¨¡å‹ä½¿ç”¨:")
                        for model, count in model_usage.items():
                            print(f"    {model}: {count} æ¬¡")
                    
                    continue
                
                if query.lower() == 'switch':
                    # åˆ‡æ¢æ¨¡å‹
                    available_models = rag_system.list_available_models()
                    selected_model = ui.get_model_selection(available_models)
                    
                    if selected_model and selected_model != 'auto':
                        if rag_system.switch_model(selected_model):
                            print(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {selected_model}")
                        else:
                            print("æ¨¡å‹åˆ‡æ¢å¤±è´¥")
                    continue
                
                if not rag_system.knowledge_base_initialized:
                    print("çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè®¾ç½®æ–‡æ¡£")
                    continue
                
                # è·å–æ£€ç´¢æ¨¡å¼
                retrieval_mode = ui.get_retrieval_mode()
                
                # è·å–ç”Ÿæˆè®¾ç½®
                generation_settings = ui.get_generation_settings()
                
                # æ‰§è¡ŒæŸ¥è¯¢
                result = rag_system.enhanced_query_v2(
                    query=query,
                    retrieval_mode=retrieval_mode,
                    generation_settings=generation_settings
                )
                
                # æ˜¾ç¤ºç»“æœ
                ui.display_query_result_v2(result)
                
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"æŸ¥è¯¢å¤„ç†é”™è¯¯: {e}")
                continue
        
        print("\næ„Ÿè°¢ä½¿ç”¨å¢å¼ºRAGç³»ç»ŸV2!")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print("\næœ€ç»ˆç»Ÿè®¡:")
        perf_stats = rag_system.get_performance_stats()
        model_stats = rag_system.get_model_stats()
        
        if model_stats.get('total_requests', 0) > 0:
            success_rate = model_stats.get('successful_requests', 0) / model_stats.get('total_requests', 1) * 100
            avg_time = model_stats.get('total_time', 0) / model_stats.get('total_requests', 1)
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
        
    except Exception as e:
        print(f"ç³»ç»Ÿè¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
