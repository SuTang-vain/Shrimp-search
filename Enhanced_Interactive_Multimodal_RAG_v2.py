"""
å¢å¼ºçš„äº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ v2.0
é›†æˆå¤šç«¯æ¨¡å‹é€‰æ‹©åŠŸèƒ½ï¼Œæ”¯æŒModelScopeè¿œç¨‹APIå’ŒOllamaæœ¬åœ°éƒ¨ç½²
"""

import os
import sys
import json
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# å¯¼å…¥V2å¢å¼ºæ¨¡å—
from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2, GenerationResult
from enhanced_user_interface_v2 import EnhancedUserInterfaceV2
from model_manager import ModelManager
from ollama_interface import OllamaInterface

# å¯¼å…¥åŸæœ‰å¢å¼ºæ¨¡å—
from enhanced_multimodal_processor import EnhancedMultimodalProcessor
from enhanced_web_research import EnhancedWebResearchSystem
from performance_monitor import PerformanceMonitor, PerformanceContext
from enhanced_document_manager import EnhancedDocumentManager, DocumentChunk

# åŸºç¡€ä¾èµ–
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# GPUæ”¯æŒæ£€æµ‹
try:
    import torch
    TORCH_AVAILABLE = True
    GPU_AVAILABLE = torch.cuda.is_available()
    if GPU_AVAILABLE:
        GPU_COUNT = torch.cuda.device_count()
        GPU_NAME = torch.cuda.get_device_name(0) if GPU_COUNT > 0 else "Unknown"
    else:
        GPU_COUNT = 0
        GPU_NAME = "None"
except ImportError:
    print("âš  PyTorchåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
    TORCH_AVAILABLE = False
    GPU_AVAILABLE = False
    GPU_COUNT = 0
    GPU_NAME = "None"

# åµŒå…¥æ¨¡å‹
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    print("âš  sentence-transformersåº“æœªå®‰è£…")
    EMBEDDING_AVAILABLE = False

class EnhancedVectorRetrieverV2:
    """å¢å¼ºçš„å‘é‡æ£€ç´¢å™¨V2 - æ”¯æŒå¤šæ¨¡å‹åµŒå…¥å’ŒGPUåŠ é€Ÿ"""
    
    def __init__(self, embedding_model, performance_monitor: Optional[PerformanceMonitor] = None, device: str = 'cpu'):
        self.embedding_model = embedding_model
        self.performance_monitor = performance_monitor
        self.device = device
        self.documents = []
        self.doc_metadata = []
        self.document_ids = []
        self.doc_embeddings = None
        print(f"å¢å¼ºå‘é‡æ£€ç´¢å™¨V2åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {device})")
    
    def add_documents(self, texts: List[str], metadata: List[Dict[str, Any]], 
                     batch_size: int = 32, show_progress: bool = True):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨"""
        if not texts:
            return
        
        operation_name = "add_documents_v2"
        additional_data = {"document_count": len(texts), "batch_size": batch_size}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                self._add_documents_impl(texts, metadata, batch_size, show_progress)
        else:
            self._add_documents_impl(texts, metadata, batch_size, show_progress)
    
    def _add_documents_impl(self, texts: List[str], metadata: List[Dict[str, Any]], 
                           batch_size: int = 32, show_progress: bool = True):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£çš„å®ç°"""
        import time
        from tqdm import tqdm
        
        try:
            start_time = time.time()
            total_docs = len(texts)
            
            # ç”Ÿæˆæ–‡æ¡£ID
            new_ids = [f"doc_{len(self.documents) + i}" for i in range(total_docs)]
            
            # å­˜å‚¨æ–‡æ¡£
            self.documents.extend(texts)
            self.doc_metadata.extend(metadata)
            self.document_ids.extend(new_ids)
            
            # æ‰¹é‡è®¡ç®—åµŒå…¥
            print(f"æ­£åœ¨æ‰¹é‡è®¡ç®— {total_docs} ä¸ªæ–‡æ¡£çš„åµŒå…¥å‘é‡... (è®¾å¤‡: {self.device}, æ‰¹æ¬¡å¤§å°: {batch_size})")
            
            all_embeddings = []
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ‰¹é‡å¤„ç†è¿›åº¦
            if show_progress and total_docs > batch_size:
                batches = [texts[i:i + batch_size] for i in range(0, total_docs, batch_size)]
                progress_bar = tqdm(batches, desc="è®¡ç®—åµŒå…¥å‘é‡", unit="batch")
                
                for batch in progress_bar:
                    batch_embeddings = self.embedding_model.encode(batch, device=self.device, show_progress_bar=False)
                    all_embeddings.append(batch_embeddings)
                    progress_bar.set_postfix({"å·²å¤„ç†": len(all_embeddings) * batch_size})
            else:
                # å°æ‰¹é‡æˆ–ä¸æ˜¾ç¤ºè¿›åº¦æ—¶ç›´æ¥å¤„ç†
                for i in range(0, total_docs, batch_size):
                    batch = texts[i:i + batch_size]
                    batch_embeddings = self.embedding_model.encode(batch, device=self.device, show_progress_bar=False)
                    all_embeddings.append(batch_embeddings)
            
            # åˆå¹¶æ‰€æœ‰åµŒå…¥å‘é‡
            if all_embeddings:
                new_embeddings = np.vstack(all_embeddings)
            else:
                new_embeddings = np.array([])
            
            # æ›´æ–°æ–‡æ¡£åµŒå…¥
            if self.doc_embeddings is None:
                self.doc_embeddings = new_embeddings
            else:
                self.doc_embeddings = np.vstack([self.doc_embeddings, new_embeddings])
            
            # æ€§èƒ½ç»Ÿè®¡
            end_time = time.time()
            processing_time = end_time - start_time
            docs_per_second = total_docs / processing_time if processing_time > 0 else 0
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {total_docs} ä¸ªæ–‡æ¡£åˆ°æ£€ç´¢å™¨")
            print(f"ğŸ“Š å¤„ç†è€—æ—¶: {processing_time:.2f}s, å¤„ç†é€Ÿåº¦: {docs_per_second:.1f} docs/s")
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def query(self, query_text: str, top_k: int = 5, filter_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸å…³æ–‡æ¡£"""
        if not self.documents or self.doc_embeddings is None:
            print("æ£€ç´¢å™¨ä¸­æ²¡æœ‰æ–‡æ¡£")
            return []
        
        operation_name = "vector_query_v2"
        additional_data = {"query_length": len(query_text), "top_k": top_k, "filter_type": filter_type}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                return self._query_impl(query_text, top_k, filter_type)
        else:
            return self._query_impl(query_text, top_k, filter_type)
    
    def _query_impl(self, query_text: str, top_k: int, filter_type: Optional[str]) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å®ç°"""
        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query_text], device=self.device)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
            
            # è·å–top_kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k * 2]
            
            results = []
            for idx in top_indices:
                if idx >= len(self.documents):
                    continue
                    
                doc_type = self.doc_metadata[idx].get('type', 'unknown')
                if filter_type and doc_type != filter_type:
                    continue
                
                results.append({
                    'text': self.documents[idx],
                    'similarity': float(similarities[idx]),
                    'metadata': self.doc_metadata[idx],
                    'id': self.document_ids[idx]
                })
                
                if len(results) >= top_k:
                    break
            
            print(f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
            
        except Exception as e:
            print(f"æŸ¥è¯¢å¤±è´¥: {e}")
            return []

class EnhancedRAGSystemV2:
    """å¢å¼ºçš„RAGç³»ç»ŸV2 - é›†æˆå¤šç«¯æ¨¡å‹é€‰æ‹©"""
    
    def __init__(self, api_key: Optional[str] = None, 
                 enable_performance_monitoring: bool = True,
                 cache_dir: str = "document_cache"):
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor(enable_performance_monitoring) if enable_performance_monitoring else None
        
        try:
            print("ğŸš€ åˆå§‹åŒ–å¢å¼ºRAGç³»ç»ŸV2...")
            
            # æ˜¾ç¤ºGPUçŠ¶æ€ä¿¡æ¯
            self._display_gpu_status()
            
            # è®¾ç½®è®¡ç®—è®¾å¤‡
            self.device = self._setup_device()
            
            # åˆå§‹åŒ–V2 LLMæ¥å£
            self.llm = EnhancedLLMInterfaceV2(api_key=api_key)
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if not EMBEDDING_AVAILABLE:
                raise ImportError("sentence-transformersä¸å¯ç”¨")
            
            print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹åˆ°è®¾å¤‡: {self.device}")
            self.embedding_model = SentenceTransformer('intfloat/e5-large-v2', device=self.device)
            print(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.device})")
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.vector_retriever = EnhancedVectorRetrieverV2(
                self.embedding_model, 
                self.performance_monitor,
                device=self.device
            )
            
            # åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨
            self.document_manager = EnhancedDocumentManager(
                cache_dir=cache_dir,
                enable_cache=True,
                max_cache_size_mb=1000
            )
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨
            self.multimodal_processor = EnhancedMultimodalProcessor()
            
            # åˆå§‹åŒ–ç½‘é¡µç ”ç©¶ç³»ç»Ÿ
            self.web_research_system = EnhancedWebResearchSystem()
            
            # ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = False
            self.multimodal_content_store = {}
            self.current_sources = []
            self.document_chunks_store = {}
            
            print("å¢å¼ºRAGç³»ç»ŸV2åˆå§‹åŒ–å®Œæˆ!")
            
        except Exception as e:
            print(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _display_gpu_status(self):
        """æ˜¾ç¤ºGPUçŠ¶æ€ä¿¡æ¯"""
        print("\nğŸ’» è®¡ç®—è®¾å¤‡çŠ¶æ€:")
        print("=" * 40)
        
        if TORCH_AVAILABLE:
            print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
            print(f"CUDAå¯ç”¨: {'âœ… æ˜¯' if GPU_AVAILABLE else 'âŒ å¦'}")
            
            if GPU_AVAILABLE:
                print(f"GPUæ•°é‡: {GPU_COUNT}")
                print(f"GPUåç§°: {GPU_NAME}")
                print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
                for i in range(GPU_COUNT):
                    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    print(f"GPU {i} æ€»å†…å­˜: {total_memory:.1f} GB")
            else:
                print("å°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        else:
            print("PyTorchæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒGPUæ£€æµ‹")
        
        print("=" * 40)
    
    def _setup_device(self) -> str:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if TORCH_AVAILABLE and GPU_AVAILABLE:
            device = 'cuda'
            print(f"âœ… å¯ç”¨GPUåŠ é€Ÿè®¡ç®— (è®¾å¤‡: {device})")
        else:
            device = 'cpu'
            if TORCH_AVAILABLE:
                print("âš ï¸ GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®¡ç®—")
            else:
                print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨CPUè®¡ç®—")
        
        return device
    
    def setup_model(self) -> bool:
        """è®¾ç½®æ¨¡å‹"""
        try:
            available_models = self.llm.list_available_models()
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œå°è¯•è‡ªåŠ¨é€‰æ‹©
            if not any(models for models in available_models.values()):
                print("æ²¡æœ‰å¯ç”¨æ¨¡å‹")
                return False
            
            # æ˜¾ç¤ºæ¨¡å‹é€‰æ‹©ç•Œé¢
            ui = EnhancedUserInterfaceV2()
            selected_model = ui.get_model_selection(available_models)
            
            if not selected_model:
                print("æœªé€‰æ‹©æ¨¡å‹")
                return False
            
            if selected_model == 'auto':
                # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
                selected_model = self.llm.auto_select_best_model()
                if not selected_model:
                    print("è‡ªåŠ¨é€‰æ‹©æ¨¡å‹å¤±è´¥")
                    return False
            else:
                # æ‰‹åŠ¨é€‰æ‹©æ¨¡å‹
                if not self.llm.set_model(selected_model):
                    print(f"è®¾ç½®æ¨¡å‹å¤±è´¥: {selected_model}")
                    return False
            
            # æµ‹è¯•æ¨¡å‹
            print("æ­£åœ¨æµ‹è¯•æ¨¡å‹...")
            test_result = self.llm.test_model()
            ui.display_test_result(test_result)
            
            return test_result['success']
            
        except Exception as e:
            print(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def setup_knowledge_base(self, sources: List[str] = None, source_type: str = "path", 
                           force_reprocess: bool = False, async_mode: bool = False):
        """è®¾ç½®çŸ¥è¯†åº“ - æ”¯æŒå¤šæ–‡æ¡£å’Œå¢é‡æ›´æ–°"""
        if sources is None:
            sources = []
        
        operation_name = "setup_knowledge_base_v2"
        additional_data = {"source_type": source_type, "source_count": len(sources), "async_mode": async_mode}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                if async_mode:
                    return self._setup_knowledge_base_async(sources, source_type, force_reprocess)
                else:
                    self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
        else:
            if async_mode:
                return self._setup_knowledge_base_async(sources, source_type, force_reprocess)
            else:
                self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
    
    def _setup_knowledge_base_impl(self, sources: List[str], source_type: str, 
                                 force_reprocess: bool):
        """çŸ¥è¯†åº“è®¾ç½®å®ç°"""
        import time
        from tqdm import tqdm
        
        try:
            start_time = time.time()
            print(f"ğŸš€ æ­£åœ¨è®¾ç½®çŸ¥è¯†åº“...")
            print(f"ğŸ“Š æ–‡æ¡£æ•°é‡: {len(sources)}")
            print(f"ğŸ“‚ æ¥æºç±»å‹: {source_type}")
            print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†: {'æ˜¯' if force_reprocess else 'å¦'}")
            print("="*60)
            
            all_texts_to_embed = []
            all_metadata_to_embed = []
            total_chunks = 0
            processing_stats = {"text": 0, "image": 0, "table": 0, "other": 0}
            failed_sources = []
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ–‡æ¡£å¤„ç†è¿›åº¦
            progress_bar = tqdm(sources, desc="å¤„ç†æ–‡æ¡£", unit="doc")
            
            for i, source in enumerate(progress_bar):
                progress_bar.set_description(f"å¤„ç†æ–‡æ¡£ [{i+1}/{len(sources)}]")
                
                # ç¡®ä¿sourceæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¸æ˜¯åˆ—è¡¨
                if isinstance(source, list):
                    print(f"âš ï¸ è­¦å‘Š: æºæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ : {source}")
                    if source:
                        source = source[0]
                    else:
                        print("â­ï¸ è·³è¿‡ç©ºåˆ—è¡¨æº")
                        continue
                
                # éªŒè¯è·¯å¾„å­˜åœ¨
                if source_type == "path" and not os.path.exists(source):
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {source}")
                    failed_sources.append(source)
                    continue
                
                try:
                    doc_start_time = time.time()
                    
                    # ä½¿ç”¨ä¼˜åŒ–çš„æ–‡æ¡£ç®¡ç†å™¨å¤„ç†æ–‡æ¡£
                    if source_type == "path":
                        # ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†æ–¹æ³•
                        document_chunks = self.document_manager._process_document_optimized(
                            source, 
                            extract_images=False,  # é»˜è®¤ä¸æå–å›¾åƒä»¥æå‡é€Ÿåº¦
                            extract_tables=True,   # ä¿ç•™è¡¨æ ¼æå–
                            force_reprocess=force_reprocess
                        )
                    else:
                        # å¯¹äºURLï¼Œå…ˆä¸‹è½½å†å¤„ç†
                        document_chunks = self._process_url_document(source, force_reprocess)
                    
                    # å­˜å‚¨æ–‡æ¡£å—
                    self.document_chunks_store[source] = document_chunks
                    
                    # è½¬æ¢ä¸ºå‘é‡æ£€ç´¢æ ¼å¼
                    for chunk in document_chunks:
                        all_texts_to_embed.append(chunk.content)
                        
                        # å¢å¼ºå…ƒæ•°æ®
                        enhanced_metadata = chunk.metadata.copy()
                        enhanced_metadata.update({
                            'chunk_id': chunk.chunk_id,
                            'chunk_type': chunk.chunk_type,
                            'source_file': source
                        })
                        all_metadata_to_embed.append(enhanced_metadata)
                        
                        # ç»Ÿè®¡
                        chunk_type = chunk.chunk_type
                        if chunk_type in processing_stats:
                            processing_stats[chunk_type] += 1
                        else:
                            processing_stats["other"] += 1
                    
                    total_chunks += len(document_chunks)
                    doc_time = time.time() - doc_start_time
                    
                    # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                    progress_bar.set_postfix({
                        "å—æ•°": len(document_chunks),
                        "è€—æ—¶": f"{doc_time:.1f}s",
                        "æ€»å—æ•°": total_chunks
                    })
                    
                except Exception as e:
                    print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
                    failed_sources.append(source)
                    continue
            
            progress_bar.close()
            
            if not all_texts_to_embed:
                raise ValueError("âŒ æœªèƒ½ä»ä»»ä½•æ–‡æ¡£ä¸­æå–åˆ°å†…å®¹")
            
            # æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡æ–¹æ³•ï¼‰
            print(f"\nğŸ” æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
            embed_start_time = time.time()
            
            self.vector_retriever.add_documents(
                all_texts_to_embed, 
                all_metadata_to_embed,
                batch_size=64,  # å¢å¤§æ‰¹æ¬¡å¤§å°ä»¥æå‡æ•ˆç‡
                show_progress=True
            )
            
            embed_time = time.time() - embed_start_time
            
            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = True
            self.current_sources = sources
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            
            print(f"\nâœ… çŸ¥è¯†åº“è®¾ç½®å®Œæˆ!")
            print("="*60)
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"   â€¢ æ€»æ–‡æ¡£æ•°: {len(sources)}")
            print(f"   â€¢ æˆåŠŸå¤„ç†: {len(sources) - len(failed_sources)}")
            print(f"   â€¢ å¤±è´¥æ–‡æ¡£: {len(failed_sources)}")
            print(f"   â€¢ æ€»æ–‡æ¡£å—: {total_chunks}")
            print(f"   â€¢ å†…å®¹åˆ†å¸ƒ: æ–‡æœ¬({processing_stats['text']}) å›¾åƒ({processing_stats['image']}) è¡¨æ ¼({processing_stats['table']}) å…¶ä»–({processing_stats['other']})")
            print(f"â±ï¸ æ€§èƒ½ç»Ÿè®¡:")
            print(f"   â€¢ æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"   â€¢ æ–‡æ¡£å¤„ç†: {total_time - embed_time:.2f}s")
            print(f"   â€¢ å‘é‡åŒ–: {embed_time:.2f}s")
            print(f"   â€¢ å¤„ç†é€Ÿåº¦: {len(sources) / total_time:.1f} docs/s")
            
            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            try:
                cache_stats = self.document_manager.get_cache_stats()
                print(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {cache_stats['cached_documents']} ä¸ªæ–‡æ¡£, {cache_stats['total_size_mb']:.1f}MB")
            except:
                pass
            
            if failed_sources:
                print(f"âš ï¸ å¤±è´¥çš„æ–‡æ¡£: {failed_sources}")
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _setup_knowledge_base_async(self, sources: List[str], source_type: str, force_reprocess: bool):
        """å¼‚æ­¥çŸ¥è¯†åº“è®¾ç½® - åˆ†ç¦»æ–‡æ¡£é¢„å¤„ç†å’Œå‘é‡åŒ–"""
        import asyncio
        import concurrent.futures
        import time
        from tqdm import tqdm
        
        async def process_documents_async():
            """å¼‚æ­¥å¤„ç†æ–‡æ¡£"""
            try:
                start_time = time.time()
                print(f"ğŸš€ å¯åŠ¨å¼‚æ­¥çŸ¥è¯†åº“è®¾ç½®...")
                print(f"ğŸ“Š æ–‡æ¡£æ•°é‡: {len(sources)}")
                print(f"ğŸ“‚ æ¥æºç±»å‹: {source_type}")
                print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†: {'æ˜¯' if force_reprocess else 'å¦'}")
                print("="*60)
                
                # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶è¡Œæ–‡æ¡£é¢„å¤„ç†
                print("ğŸ“ ç¬¬ä¸€é˜¶æ®µï¼šå¹¶è¡Œæ–‡æ¡£é¢„å¤„ç†")
                all_document_chunks = []
                processing_stats = {"text": 0, "image": 0, "table": 0, "other": 0}
                failed_sources = []
                
                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ–‡æ¡£
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    # æäº¤æ‰€æœ‰æ–‡æ¡£å¤„ç†ä»»åŠ¡
                    future_to_source = {
                        executor.submit(self._process_single_document, source, source_type, force_reprocess): source 
                        for source in sources
                    }
                    
                    # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
                    progress_bar = tqdm(total=len(sources), desc="é¢„å¤„ç†æ–‡æ¡£", unit="doc")
                    
                    for future in concurrent.futures.as_completed(future_to_source):
                        source = future_to_source[future]
                        try:
                            document_chunks = future.result()
                            if document_chunks:
                                all_document_chunks.extend(document_chunks)
                                self.document_chunks_store[source] = document_chunks
                                
                                # ç»Ÿè®¡
                                for chunk in document_chunks:
                                    chunk_type = chunk.chunk_type
                                    if chunk_type in processing_stats:
                                        processing_stats[chunk_type] += 1
                                    else:
                                        processing_stats["other"] += 1
                            
                            progress_bar.set_postfix({"æˆåŠŸ": len(all_document_chunks), "å¤±è´¥": len(failed_sources)})
                            
                        except Exception as e:
                            print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
                            failed_sources.append(source)
                        
                        progress_bar.update(1)
                    
                    progress_bar.close()
                
                if not all_document_chunks:
                    raise ValueError("âŒ æœªèƒ½ä»ä»»ä½•æ–‡æ¡£ä¸­æå–åˆ°å†…å®¹")
                
                preprocess_time = time.time() - start_time
                print(f"âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {preprocess_time:.2f}s")
                
                # ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å‘é‡åŒ–
                print("\nğŸ” ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å‘é‡åŒ–")
                embed_start_time = time.time()
                
                # å‡†å¤‡å‘é‡åŒ–æ•°æ®
                all_texts_to_embed = []
                all_metadata_to_embed = []
                
                for chunk in all_document_chunks:
                    all_texts_to_embed.append(chunk.content)
                    
                    # å¢å¼ºå…ƒæ•°æ®
                    enhanced_metadata = chunk.metadata.copy()
                    enhanced_metadata.update({
                        'chunk_id': chunk.chunk_id,
                        'chunk_type': chunk.chunk_type,
                        'source_file': chunk.metadata.get('source', 'unknown')
                    })
                    all_metadata_to_embed.append(enhanced_metadata)
                
                # æ‰¹é‡å‘é‡åŒ–
                self.vector_retriever.add_documents(
                    all_texts_to_embed, 
                    all_metadata_to_embed,
                    batch_size=128,  # å¼‚æ­¥æ¨¡å¼ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡
                    show_progress=True
                )
                
                embed_time = time.time() - embed_start_time
                total_time = time.time() - start_time
                
                # æ›´æ–°ç³»ç»ŸçŠ¶æ€
                self.knowledge_base_initialized = True
                self.current_sources = sources
                
                # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                print(f"\nâœ… å¼‚æ­¥çŸ¥è¯†åº“è®¾ç½®å®Œæˆ!")
                print("="*60)
                print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
                print(f"   â€¢ æ€»æ–‡æ¡£æ•°: {len(sources)}")
                print(f"   â€¢ æˆåŠŸå¤„ç†: {len(sources) - len(failed_sources)}")
                print(f"   â€¢ å¤±è´¥æ–‡æ¡£: {len(failed_sources)}")
                print(f"   â€¢ æ€»æ–‡æ¡£å—: {len(all_document_chunks)}")
                print(f"   â€¢ å†…å®¹åˆ†å¸ƒ: æ–‡æœ¬({processing_stats['text']}) å›¾åƒ({processing_stats['image']}) è¡¨æ ¼({processing_stats['table']}) å…¶ä»–({processing_stats['other']})")
                print(f"â±ï¸ æ€§èƒ½ç»Ÿè®¡:")
                print(f"   â€¢ æ€»è€—æ—¶: {total_time:.2f}s")
                print(f"   â€¢ æ–‡æ¡£é¢„å¤„ç†: {preprocess_time:.2f}s ({preprocess_time/total_time*100:.1f}%)")
                print(f"   â€¢ å‘é‡åŒ–: {embed_time:.2f}s ({embed_time/total_time*100:.1f}%)")
                print(f"   â€¢ å¤„ç†é€Ÿåº¦: {len(sources) / total_time:.1f} docs/s")
                
                if failed_sources:
                    print(f"âš ï¸ å¤±è´¥çš„æ–‡æ¡£: {failed_sources}")
                
                return {
                    "success": True,
                    "total_documents": len(sources),
                    "processed_documents": len(sources) - len(failed_sources),
                    "total_chunks": len(all_document_chunks),
                    "processing_time": total_time,
                    "failed_sources": failed_sources
                }
                
            except Exception as e:
                print(f"âŒ å¼‚æ­¥çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
                return {"success": False, "error": str(e)}
        
        # è¿è¡Œå¼‚æ­¥å¤„ç†
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(process_documents_async())
    
    def _process_single_document(self, source: str, source_type: str, force_reprocess: bool):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰"""
        try:
            # ç¡®ä¿sourceæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œä¸æ˜¯åˆ—è¡¨
            if isinstance(source, list):
                if source:
                    source = source[0]
                else:
                    return None
            
            # éªŒè¯è·¯å¾„å­˜åœ¨
            if source_type == "path" and not os.path.exists(source):
                return None
            
            # ä½¿ç”¨ä¼˜åŒ–çš„æ–‡æ¡£ç®¡ç†å™¨å¤„ç†æ–‡æ¡£
            if source_type == "path":
                document_chunks = self.document_manager._process_document_optimized(
                    source, 
                    extract_images=False,  # é»˜è®¤ä¸æå–å›¾åƒä»¥æå‡é€Ÿåº¦
                    extract_tables=True,   # ä¿ç•™è¡¨æ ¼æå–
                    force_reprocess=force_reprocess
                )
            else:
                # å¯¹äºURLï¼Œå…ˆä¸‹è½½å†å¤„ç†
                document_chunks = self._process_url_document(source, force_reprocess)
            
            return document_chunks
            
        except Exception as e:
            print(f"âŒ å•æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
            return None
    
    def _process_url_document(self, url: str, force_reprocess: bool) -> List[DocumentChunk]:
        """å¤„ç†URLæ–‡æ¡£"""
        import requests
        import tempfile
        from urllib.parse import urlparse
        
        try:
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            file_ext = os.path.splitext(parsed_url.path)[1]
            if not file_ext:
                file_ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=file_ext, delete=False) as temp_file:
                temp_file.write(response.content)
                temp_path = temp_file.name
            
            try:
                # å¤„ç†æ–‡æ¡£
                chunks = self.document_manager.process_document(temp_path, force_reprocess)
                
                # æ›´æ–°å…ƒæ•°æ®ä¸­çš„æºä¿¡æ¯
                for chunk in chunks:
                    chunk.metadata['original_url'] = url
                    chunk.metadata['source'] = url
                
                return chunks
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            print(f"URLæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return []
    
    def enhanced_query_v2(self, query: str, retrieval_mode: str = "å¿«é€Ÿæ£€ç´¢", 
                         generation_settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºæŸ¥è¯¢V2 - æ”¯æŒè‡ªå®šä¹‰ç”Ÿæˆè®¾ç½®"""
        if not self.knowledge_base_initialized:
            raise RuntimeError("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        if not self.llm.current_backend:
            raise RuntimeError("æœªè®¾ç½®æ¨¡å‹")
        
        operation_name = f"enhanced_query_v2_{retrieval_mode}"
        additional_data = {"query_length": len(query), "mode": retrieval_mode}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                return self._enhanced_query_v2_impl(query, retrieval_mode, generation_settings)
        else:
            return self._enhanced_query_v2_impl(query, retrieval_mode, generation_settings)
    
    def _enhanced_query_v2_impl(self, query: str, retrieval_mode: str, 
                               generation_settings: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢V2å®ç°"""
        try:
            print(f"\næ‰§è¡Œ{retrieval_mode}: {query}")
            print("="*60)
            
            # è®¾ç½®é»˜è®¤ç”Ÿæˆå‚æ•°
            gen_settings = generation_settings or {}
            max_tokens = gen_settings.get('max_tokens', 800)
            temperature = gen_settings.get('temperature', 0.7)
            
            if retrieval_mode == "å¿«é€Ÿæ£€ç´¢":
                return self._quick_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "æ·±åº¦æ£€ç´¢":
                return self._deep_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "ä¸»é¢˜æ£€ç´¢":
                return self._topic_retrieval_v2(query, max_tokens, temperature)
            elif retrieval_mode == "æ™ºèƒ½æ£€ç´¢":
                return self._smart_retrieval_v2(query, max_tokens, temperature)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢æ¨¡å¼: {retrieval_mode}")
                
        except Exception as e:
            print(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}"}
    
    def _quick_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """å¿«é€Ÿæ£€ç´¢V2"""
        print("æ‰§è¡Œå¿«é€Ÿæ£€ç´¢...")
        
        # ç›´æ¥å‘é‡æ£€ç´¢
        retrieved_docs = self.vector_retriever.query(query, top_k=3, filter_type='text')
        
        if not retrieved_docs:
            return {"error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"}
        
        # ç”Ÿæˆç­”æ¡ˆ
        context_texts = [doc['text'] for doc in retrieved_docs]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç®€æ´æ˜ç¡®çš„ç­”æ¡ˆï¼š"""
        
        # ä½¿ç”¨V2æ¥å£ç”Ÿæˆ
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'retrieved_docs': retrieved_docs,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'å¿«é€Ÿæ£€ç´¢V2',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _deep_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """æ·±åº¦æ£€ç´¢V2"""
        print("æ‰§è¡Œæ·±åº¦æ£€ç´¢...")
        
        # æ­¥éª¤1: æŸ¥è¯¢é‡å†™
        print("æ­¥éª¤1: æŸ¥è¯¢é‡å†™")
        rewritten_query = self._query_rewriting_v2(query)
        
        # æ­¥éª¤2: HyDEç”Ÿæˆ
        print("æ­¥éª¤2: HyDEå‡è®¾æ–‡æ¡£ç”Ÿæˆ")
        hyde_doc = self._hyde_generation_v2(rewritten_query)
        
        # æ­¥éª¤3: å¤šè·¯æ£€ç´¢
        print("æ­¥éª¤3: å¤šè·¯æ£€ç´¢")
        all_results = []
        
        # åŸå§‹æŸ¥è¯¢æ£€ç´¢
        original_results = self.vector_retriever.query(query, top_k=5)
        all_results.append(original_results)
        
        # é‡å†™æŸ¥è¯¢æ£€ç´¢
        rewritten_results = self.vector_retriever.query(rewritten_query, top_k=5)
        all_results.append(rewritten_results)
        
        # HyDEæ–‡æ¡£æ£€ç´¢
        hyde_results = self.vector_retriever.query(hyde_doc, top_k=5)
        all_results.append(hyde_results)
        
        # å¤šæ¨¡æ€æ£€ç´¢
        image_results = self.vector_retriever.query(query, top_k=3, filter_type='image')
        if image_results:
            all_results.append(image_results)
        
        table_results = self.vector_retriever.query(query, top_k=3, filter_type='table')
        if table_results:
            all_results.append(table_results)
        
        # æ­¥éª¤4: RRFèåˆ
        print("æ­¥éª¤4: RRFç»“æœèåˆ")
        final_docs = self._rrf_fusion(all_results)
        
        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        print("æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        context_texts = [doc['text'] for doc in final_docs[:5]]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹
2. æä¾›å‡†ç¡®è¯¦ç»†çš„ç­”æ¡ˆ
3. å¼•ç”¨å…·ä½“æ–‡æ¡£å†…å®¹
4. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„

è¯·æä¾›è¯¦ç»†ç­”æ¡ˆï¼š"""
        
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'rewritten_query': rewritten_query,
            'hyde_doc': hyde_doc,
            'retrieved_docs': final_docs,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'æ·±åº¦æ£€ç´¢V2(é‡å†™+HyDE+RRF)',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _topic_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """ä¸»é¢˜æ£€ç´¢V2"""
        print("æ‰§è¡Œä¸»é¢˜æ£€ç´¢...")
        
        # æ­¥éª¤1: PDFæ£€ç´¢
        print("æ­¥éª¤1: PDFæ–‡æ¡£æ£€ç´¢")
        pdf_docs = self.vector_retriever.query(query, top_k=5)
        
        # æ­¥éª¤2: ç½‘é¡µç ”ç©¶
        print("æ­¥éª¤2: ç½‘é¡µç ”ç©¶")
        web_research_result = self.web_research_system.research_topic(query, max_pages=3)
        
        # æ­¥éª¤3: ç»¼åˆåˆ†æ
        print("æ­¥éª¤3: ç»¼åˆåˆ†æ")
        
        # åˆå¹¶å†…å®¹
        combined_context = ""
        
        # æ·»åŠ PDFå†…å®¹
        if pdf_docs:
            pdf_context = "\n".join([doc['text'] for doc in pdf_docs])
            combined_context += f"PDFæ–‡æ¡£å†…å®¹ï¼š\n{pdf_context}\n\n"
        
        # æ·»åŠ ç½‘é¡µå†…å®¹
        web_analysis = web_research_result.get('analysis', '')
        if web_analysis and not web_analysis.startswith("æœªèƒ½è·å–"):
            combined_context += f"ç½‘é¡µç ”ç©¶å†…å®¹ï¼š\n{web_analysis}\n\n"
        
        if not combined_context.strip():
            combined_context = f"å…³äº'{query}'çš„ä¿¡æ¯ä¸»è¦æ¥è‡ªPDFæ–‡æ¡£ï¼Œç½‘é¡µç ”ç©¶åŠŸèƒ½å½“å‰å—é™ã€‚"
        
        # ç”Ÿæˆç»¼åˆç­”æ¡ˆ
        prompt = f"""åŸºäºä»¥ä¸‹PDFæ–‡æ¡£å’Œç½‘é¡µç ”ç©¶ä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼š

{combined_context}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç»¼åˆæ€§çš„è¯¦ç»†ç­”æ¡ˆï¼Œæ•´åˆPDFå’Œç½‘é¡µä¿¡æ¯ï¼š"""
        
        result = self.llm.generate(prompt, max_tokens=max_tokens, temperature=temperature)
        
        return {
            'original_query': query,
            'pdf_docs': pdf_docs,
            'web_research': web_research_result,
            'final_answer': result.content if not result.error else f"ç”Ÿæˆå¤±è´¥: {result.error}",
            'retrieval_method': 'ä¸»é¢˜æ£€ç´¢V2(PDF+ç½‘é¡µ)',
            'model_info': {
                'key': result.model_key,
                'type': result.model_type,
                'generation_time': result.generation_time
            }
        }
    
    def _smart_retrieval_v2(self, query: str, max_tokens: int, temperature: float) -> Dict[str, Any]:
        """æ™ºèƒ½æ£€ç´¢V2 - è‡ªé€‚åº”é€‰æ‹©æœ€ä½³ç­–ç•¥"""
        print("æ‰§è¡Œæ™ºèƒ½æ£€ç´¢...")
        
        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
        query_complexity = self._analyze_query_complexity(query)
        
        print(f"æŸ¥è¯¢å¤æ‚åº¦åˆ†æ: {query_complexity}")
        
        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©ç­–ç•¥
        if query_complexity['complexity_score'] < 0.3:
            print("é€‰æ‹©å¿«é€Ÿæ£€ç´¢ç­–ç•¥")
            return self._quick_retrieval_v2(query, max_tokens, temperature)
        elif query_complexity['complexity_score'] < 0.7:
            print("é€‰æ‹©æ·±åº¦æ£€ç´¢ç­–ç•¥")
            return self._deep_retrieval_v2(query, max_tokens, temperature)
        else:
            print("é€‰æ‹©ä¸»é¢˜æ£€ç´¢ç­–ç•¥")
            return self._topic_retrieval_v2(query, max_tokens, temperature)
    
    def _analyze_query_complexity(self, query: str) -> Dict[str, Any]:
        """åˆ†ææŸ¥è¯¢å¤æ‚åº¦"""
        complexity_indicators = {
            'length': len(query.split()),
            'has_multiple_questions': '?' in query and query.count('?') > 1,
            'has_comparison': any(word in query.lower() for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'vs', 'åŒºåˆ«', 'ä¸åŒ']),
            'has_analysis': any(word in query.lower() for word in ['åˆ†æ', 'è§£é‡Š', 'åŸå› ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•']),
            'has_synthesis': any(word in query.lower() for word in ['ç»¼åˆ', 'æ€»ç»“', 'æ¦‚æ‹¬', 'æ•´ä½“'])
        }
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        score = 0.0
        if complexity_indicators['length'] > 10:
            score += 0.2
        if complexity_indicators['has_multiple_questions']:
            score += 0.3
        if complexity_indicators['has_comparison']:
            score += 0.2
        if complexity_indicators['has_analysis']:
            score += 0.2
        if complexity_indicators['has_synthesis']:
            score += 0.3
        
        return {
            'complexity_score': min(score, 1.0),
            'indicators': complexity_indicators
        }
    
    def _query_rewriting_v2(self, query: str) -> str:
        """æŸ¥è¯¢é‡å†™V2"""
        try:
            prompt = f"""è¯·é‡å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ–‡æ¡£æ£€ç´¢ï¼š

åŸå§‹æŸ¥è¯¢ï¼š{query}

é‡å†™è¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒæ„å›¾
2. ä½¿ç”¨æ›´å…·ä½“çš„è¯æ±‡
3. é€‚åˆå‘é‡æ£€ç´¢
4. åªè¿”å›é‡å†™åçš„æŸ¥è¯¢

é‡å†™æŸ¥è¯¢ï¼š"""
            
            result = self.llm.generate(prompt, max_tokens=100, temperature=0.1)
            
            if result.error:
                print(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {result.error}")
                return query
            
            rewritten = result.content.strip()
            
            # æ¸…ç†ç»“æœ
            if rewritten and not rewritten.startswith("æŠ±æ­‰"):
                lines = rewritten.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('é‡å†™') and not line.startswith('åŸå§‹'):
                        return line
            
            return query
            
        except Exception as e:
            print(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {e}")
            return query
    
    def _hyde_generation_v2(self, query: str) -> str:
        """HyDEå‡è®¾æ–‡æ¡£ç”ŸæˆV2"""
        try:
            prompt = f"""åŸºäºæŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§æ–‡æ¡£ç‰‡æ®µï¼š

æŸ¥è¯¢ï¼š{query}

è¦æ±‚ï¼š
1. ç”Ÿæˆ200-300å­—çš„æ–‡æ¡£ç‰‡æ®µ
2. ç›´æ¥å›ç­”æŸ¥è¯¢é—®é¢˜
3. ä½¿ç”¨ä¸“ä¸šå‡†ç¡®çš„è¯­è¨€
4. åŒ…å«ç›¸å…³æŠ€æœ¯ç»†èŠ‚

å‡è®¾æ–‡æ¡£ï¼š"""
            
            result = self.llm.generate(prompt, max_tokens=300, temperature=0.3)
            
            if result.error:
                print(f"HyDEç”Ÿæˆå¤±è´¥: {result.error}")
                return query
            
            hyde_doc = result.content.strip()
            
            if hyde_doc and not hyde_doc.startswith("æŠ±æ­‰"):
                return hyde_doc
            else:
                return query
                
        except Exception as e:
            print(f"HyDEç”Ÿæˆå¤±è´¥: {e}")
            return query
    
    def _rrf_fusion(self, all_results: List[List[Dict]], k: int = 60) -> List[Dict[str, Any]]:
        """RRFèåˆå¤šä¸ªæ£€ç´¢ç»“æœ"""
        try:
            doc_scores = {}
            
            for results in all_results:
                for rank, doc in enumerate(results):
                    doc_id = doc.get('id', '')
                    if doc_id:
                        rrf_score = 1.0 / (k + rank + 1)
                        if doc_id in doc_scores:
                            doc_scores[doc_id]['score'] += rrf_score
                        else:
                            doc_scores[doc_id] = {
                                'doc': doc,
                                'score': rrf_score
                            }
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)
            
            # æ·»åŠ RRFåˆ†æ•°
            final_docs = []
            for item in sorted_docs:
                doc = item['doc'].copy()
                doc['rrf_score'] = item['score']
                final_docs.append(doc)
            
            return final_docs
            
        except Exception as e:
            print(f"RRFèåˆå¤±è´¥: {e}")
            return all_results[0] if all_results else []
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if self.performance_monitor:
            return self.performance_monitor.get_stats()
        return {}
    
    def get_model_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        return self.llm.generation_stats
    
    def switch_model(self, model_key: str) -> bool:
        """åˆ‡æ¢æ¨¡å‹"""
        return self.llm.set_model(model_key)
    
    def list_available_models(self) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return self.llm.list_available_models()
    
    def get_current_model_info(self) -> Optional[Dict[str, Any]]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        return self.llm.get_current_model()

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç³»ç»ŸåŠŸèƒ½"""
    try:
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        api_key = os.getenv('MODELSCOPE_API_KEY')
        
        print("="*60)
        print("å¢å¼ºäº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ V2.0")
        print("æ”¯æŒå¤šç«¯æ¨¡å‹é€‰æ‹© (ModelScope + Ollama)")
        print("="*60)
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        rag_system = EnhancedRAGSystemV2(
            api_key=api_key,
            enable_performance_monitoring=True
        )
        
        # è®¾ç½®æ¨¡å‹
        print("\næ­¥éª¤1: æ¨¡å‹è®¾ç½®")
        if not rag_system.setup_model():
            print("æ¨¡å‹è®¾ç½®å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹ä¿¡æ¯
        current_model = rag_system.get_current_model_info()
        if current_model:
            print(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {current_model['key']} ({current_model['provider']})")
        
        # è®¾ç½®çŸ¥è¯†åº“
        print("\næ­¥éª¤2: çŸ¥è¯†åº“è®¾ç½®")
        ui = EnhancedUserInterfaceV2()
        
        # è·å–æ–‡æ¡£è·¯å¾„
        document_sources = ui.get_document_sources()
        if not document_sources:
            print("æœªæä¾›æ–‡æ¡£ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ¼”ç¤ºæ–‡æ¡£
            document_sources = []
        
        if document_sources:
            rag_system.setup_knowledge_base(
                sources=document_sources,
                source_type="path",
                force_reprocess=False
            )
        
        # äº¤äº’å¼æŸ¥è¯¢å¾ªç¯
        print("\næ­¥éª¤3: å¼€å§‹äº¤äº’å¼æŸ¥è¯¢")
        print("è¾“å…¥ 'quit' é€€å‡ºï¼Œ'stats' æŸ¥çœ‹ç»Ÿè®¡ï¼Œ'switch' åˆ‡æ¢æ¨¡å‹")
        print("-" * 60)
        
        while True:
            try:
                query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not query:
                    continue
                
                if query.lower() == 'quit':
                    break
                
                if query.lower() == 'stats':
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    perf_stats = rag_system.get_performance_stats()
                    model_stats = rag_system.get_model_stats()
                    
                    print("\næ€§èƒ½ç»Ÿè®¡:")
                    if perf_stats:
                        for operation, stats in perf_stats.items():
                            print(f"  {operation}: {stats.get('count', 0)} æ¬¡, å¹³å‡è€—æ—¶: {stats.get('avg_time', 0):.2f}s")
                    
                    print("\næ¨¡å‹ç»Ÿè®¡:")
                    print(f"  æ€»è¯·æ±‚æ•°: {model_stats.get('total_requests', 0)}")
                    print(f"  æˆåŠŸè¯·æ±‚: {model_stats.get('successful_requests', 0)}")
                    print(f"  å¤±è´¥è¯·æ±‚: {model_stats.get('failed_requests', 0)}")
                    print(f"  æ€»è€—æ—¶: {model_stats.get('total_time', 0):.2f}s")
                    
                    model_usage = model_stats.get('model_usage', {})
                    if model_usage:
                        print("  æ¨¡å‹ä½¿ç”¨:")
                        for model, count in model_usage.items():
                            print(f"    {model}: {count} æ¬¡")
                    
                    continue
                
                if query.lower() == 'switch':
                    # åˆ‡æ¢æ¨¡å‹
                    available_models = rag_system.list_available_models()
                    selected_model = ui.get_model_selection(available_models)
                    
                    if selected_model and selected_model != 'auto':
                        if rag_system.switch_model(selected_model):
                            print(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {selected_model}")
                        else:
                            print("æ¨¡å‹åˆ‡æ¢å¤±è´¥")
                    continue
                
                if not rag_system.knowledge_base_initialized:
                    print("çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè®¾ç½®æ–‡æ¡£")
                    continue
                
                # è·å–æ£€ç´¢æ¨¡å¼
                retrieval_mode = ui.get_retrieval_mode()
                
                # è·å–ç”Ÿæˆè®¾ç½®
                generation_settings = ui.get_generation_settings()
                
                # æ‰§è¡ŒæŸ¥è¯¢
                result = rag_system.enhanced_query_v2(
                    query=query,
                    retrieval_mode=retrieval_mode,
                    generation_settings=generation_settings
                )
                
                # æ˜¾ç¤ºç»“æœ
                ui.display_query_result_v2(result)
                
            except KeyboardInterrupt:
                print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                print(f"æŸ¥è¯¢å¤„ç†é”™è¯¯: {e}")
                continue
        
        print("\næ„Ÿè°¢ä½¿ç”¨å¢å¼ºRAGç³»ç»ŸV2!")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print("\næœ€ç»ˆç»Ÿè®¡:")
        perf_stats = rag_system.get_performance_stats()
        model_stats = rag_system.get_model_stats()
        
        if model_stats.get('total_requests', 0) > 0:
            success_rate = model_stats.get('successful_requests', 0) / model_stats.get('total_requests', 1) * 100
            avg_time = model_stats.get('total_time', 0) / model_stats.get('total_requests', 1)
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
        
    except Exception as e:
        print(f"ç³»ç»Ÿè¿è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
