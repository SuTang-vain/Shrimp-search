"""
å¢å¼ºçš„æ–‡æ¡£ç®¡ç†å™¨ v2.0
æ”¯æŒæ–‡æ¡£é¢„å¤„ç†ç¼“å­˜ã€å¢é‡æ›´æ–°å’Œå¤šæ ¼å¼å¤„ç†
"""

import os
import json
import hashlib
import pickle
import time
from typing import List, Dict, Any, Optional, Set, Tuple
from pathlib import Path
import logging
from dataclasses import dataclass, asdict
from datetime import datetime

# æ–‡æ¡£å¤„ç†åº“
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
except ImportError:
    PYMUPDF_AVAILABLE = False

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    from pptx import Presentation
    PPTX_AVAILABLE = True
except ImportError:
    PPTX_AVAILABLE = False

try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

@dataclass
class DocumentMetadata:
    """æ–‡æ¡£å…ƒæ•°æ®"""
    file_path: str
    file_hash: str
    file_size: int
    last_modified: float
    processing_time: float
    chunk_count: int
    content_type: str
    format_type: str
    created_at: str
    version: str = "1.0"

@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—"""
    chunk_id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    chunk_type: str = "text"  # text, image, table, code

class EnhancedDocumentManager:
    """å¢å¼ºçš„æ–‡æ¡£ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "document_cache", 
                 enable_cache: bool = True,
                 max_cache_size_mb: int = 1000):
        self.cache_dir = Path(cache_dir)
        self.enable_cache = enable_cache
        self.max_cache_size_mb = max_cache_size_mb
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if self.enable_cache:
            self.cache_dir.mkdir(exist_ok=True)
            (self.cache_dir / "metadata").mkdir(exist_ok=True)
            (self.cache_dir / "chunks").mkdir(exist_ok=True)
            (self.cache_dir / "embeddings").mkdir(exist_ok=True)
        
        # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        self.supported_formats = {
            '.pdf': self._process_pdf,
            '.docx': self._process_docx,
            '.doc': self._process_doc,
            '.txt': self._process_txt,
            '.md': self._process_markdown,
            '.csv': self._process_csv,
            '.xlsx': self._process_excel,
            '.xls': self._process_excel,
            '.pptx': self._process_pptx,
            '.ppt': self._process_ppt,
            '.json': self._process_json,
            '.xml': self._process_xml,
            '.html': self._process_html,
            '.rtf': self._process_rtf
        }
        
        # æ–‡æ¡£ç¼“å­˜ç´¢å¼•
        self.document_index = self._load_document_index()
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        print("âœ… å¢å¼ºæ–‡æ¡£ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ“‹ æ”¯æŒæ ¼å¼: {list(self.supported_formats.keys())}")
    
    def _load_document_index(self) -> Dict[str, DocumentMetadata]:
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        index_file = self.cache_dir / "document_index.json"
        if index_file.exists():
            try:
                with open(index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return {k: DocumentMetadata(**v) for k, v in data.items()}
            except Exception as e:
                self.logger.warning(f"åŠ è½½æ–‡æ¡£ç´¢å¼•å¤±è´¥: {e}")
        return {}
    
    def _save_document_index(self):
        """ä¿å­˜æ–‡æ¡£ç´¢å¼•"""
        if not self.enable_cache:
            return
        
        index_file = self.cache_dir / "document_index.json"
        try:
            data = {k: asdict(v) for k, v in self.document_index.items()}
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡æ¡£ç´¢å¼•å¤±è´¥: {e}")
    
    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
            return ""
    
    def _is_document_cached(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç¼“å­˜ä¸”æœ‰æ•ˆ"""
        if not self.enable_cache or file_path not in self.document_index:
            return False
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return False
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_stat = os.stat(file_path)
            cached_metadata = self.document_index[file_path]
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
            if (file_stat.st_mtime != cached_metadata.last_modified or
                file_stat.st_size != cached_metadata.file_size):
                return False
            
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            cache_file = self.cache_dir / "chunks" / f"{cached_metadata.file_hash}.pkl"
            return cache_file.exists()
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def _load_cached_chunks(self, file_path: str) -> List[DocumentChunk]:
        """åŠ è½½ç¼“å­˜çš„æ–‡æ¡£å—"""
        try:
            metadata = self.document_index[file_path]
            cache_file = self.cache_dir / "chunks" / f"{metadata.file_hash}.pkl"
            
            with open(cache_file, 'rb') as f:
                chunks_data = pickle.load(f)
                return [DocumentChunk(**chunk) for chunk in chunks_data]
                
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜å—å¤±è´¥: {e}")
            return []
    
    def _save_chunks_to_cache(self, file_path: str, chunks: List[DocumentChunk], 
                             metadata: DocumentMetadata):
        """ä¿å­˜æ–‡æ¡£å—åˆ°ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        try:
            # ä¿å­˜å—æ•°æ®
            cache_file = self.cache_dir / "chunks" / f"{metadata.file_hash}.pkl"
            chunks_data = [asdict(chunk) for chunk in chunks]
            
            with open(cache_file, 'wb') as f:
                pickle.dump(chunks_data, f)
            
            # æ›´æ–°ç´¢å¼•
            self.document_index[file_path] = metadata
            self._save_document_index()
            
            print(f"âœ… æ–‡æ¡£ç¼“å­˜å·²ä¿å­˜: {len(chunks)} ä¸ªå—")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _clean_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        try:
            # è®¡ç®—ç¼“å­˜å¤§å°
            total_size = 0
            cache_files = []
            
            for cache_file in (self.cache_dir / "chunks").glob("*.pkl"):
                size = cache_file.stat().st_size
                total_size += size
                cache_files.append((cache_file, size, cache_file.stat().st_mtime))
            
            # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æœ€æ—§çš„æ–‡ä»¶
            if total_size > self.max_cache_size_mb * 1024 * 1024:
                cache_files.sort(key=lambda x: x[2])  # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                
                for cache_file, size, _ in cache_files:
                    if total_size <= self.max_cache_size_mb * 1024 * 1024:
                        break
                    
                    cache_file.unlink()
                    total_size -= size
                    print(f"ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜æ–‡ä»¶: {cache_file.name}")
                    
        except Exception as e:
            self.logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def process_document(self, file_path: str, force_reprocess: bool = False) -> List[DocumentChunk]:
        """å¤„ç†æ–‡æ¡£"""
        print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        if not force_reprocess and self._is_document_cached(file_path):
            print("âš¡ ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return self._load_cached_chunks(file_path)
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stat = os.stat(file_path)
        file_ext = Path(file_path).suffix.lower()
        
        # æ£€æŸ¥æ ¼å¼æ”¯æŒ
        if file_ext not in self.supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        # å¤„ç†æ–‡æ¡£
        start_time = time.time()
        processor = self.supported_formats[file_ext]
        chunks = processor(file_path)
        processing_time = time.time() - start_time
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = DocumentMetadata(
            file_path=file_path,
            file_hash=self._get_file_hash(file_path),
            file_size=file_stat.st_size,
            last_modified=file_stat.st_mtime,
            processing_time=processing_time,
            chunk_count=len(chunks),
            content_type=self._detect_content_type(chunks),
            format_type=file_ext,
            created_at=datetime.now().isoformat()
        )
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_chunks_to_cache(file_path, chunks, metadata)
        
        # æ¸…ç†ç¼“å­˜
        self._clean_cache()
        
        print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªå—, è€—æ—¶ {processing_time:.2f}s")
        return chunks
    
    def _detect_content_type(self, chunks: List[DocumentChunk]) -> str:
        """æ£€æµ‹å†…å®¹ç±»å‹"""
        type_counts = {}
        for chunk in chunks:
            chunk_type = chunk.chunk_type
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
        
        if not type_counts:
            return "unknown"
        
        return max(type_counts, key=type_counts.get)
    
    def _create_chunk(self, content: str, metadata: Dict[str, Any], 
                     chunk_type: str = "text") -> DocumentChunk:
        """åˆ›å»ºæ–‡æ¡£å—"""
        chunk_id = hashlib.md5(content.encode()).hexdigest()[:16]
        return DocumentChunk(
            chunk_id=chunk_id,
            content=content,
            metadata=metadata,
            chunk_type=chunk_type
        )
    
    # å„ç§æ ¼å¼çš„å¤„ç†å™¨
    def _process_pdf(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†PDFæ–‡ä»¶"""
        if not PYMUPDF_AVAILABLE:
            raise ImportError("PyMuPDFæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†PDFæ–‡ä»¶")
        
        chunks = []
        try:
            doc = fitz.open(file_path)
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # æå–æ–‡æœ¬
                text = page.get_text()
                if text.strip():
                    metadata = {
                        "page": page_num + 1,
                        "source": file_path,
                        "type": "text"
                    }
                    chunks.append(self._create_chunk(text, metadata, "text"))
                
                # æå–å›¾åƒ
                image_list = page.get_images()
                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)
                        if pix.n - pix.alpha < 4:  # ç¡®ä¿ä¸æ˜¯CMYK
                            img_data = pix.tobytes("png")
                            metadata = {
                                "page": page_num + 1,
                                "source": file_path,
                                "type": "image",
                                "image_index": img_index
                            }
                            chunks.append(self._create_chunk(f"å›¾åƒæ•°æ® (é¡µé¢ {page_num + 1})", metadata, "image"))
                        pix = None
                    except Exception as e:
                        self.logger.warning(f"æå–å›¾åƒå¤±è´¥: {e}")
                
                # æå–è¡¨æ ¼
                tables = page.find_tables()
                for table_index, table in enumerate(tables):
                    try:
                        table_data = table.extract()
                        table_text = "\n".join(["\t".join(row) for row in table_data])
                        metadata = {
                            "page": page_num + 1,
                            "source": file_path,
                            "type": "table",
                            "table_index": table_index
                        }
                        chunks.append(self._create_chunk(table_text, metadata, "table"))
                    except Exception as e:
                        self.logger.warning(f"æå–è¡¨æ ¼å¤±è´¥: {e}")
            
            doc.close()
            
        except Exception as e:
            raise Exception(f"PDFå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_docx(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†DOCXæ–‡ä»¶"""
        if not DOCX_AVAILABLE:
            raise ImportError("python-docxæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†DOCXæ–‡ä»¶")
        
        chunks = []
        try:
            doc = Document(file_path)
            
            # å¤„ç†æ®µè½
            for para_index, paragraph in enumerate(doc.paragraphs):
                if paragraph.text.strip():
                    metadata = {
                        "paragraph": para_index + 1,
                        "source": file_path,
                        "type": "text"
                    }
                    chunks.append(self._create_chunk(paragraph.text, metadata, "text"))
            
            # å¤„ç†è¡¨æ ¼
            for table_index, table in enumerate(doc.tables):
                table_data = []
                for row in table.rows:
                    row_data = [cell.text for cell in row.cells]
                    table_data.append(row_data)
                
                table_text = "\n".join(["\t".join(row) for row in table_data])
                metadata = {
                    "table": table_index + 1,
                    "source": file_path,
                    "type": "table"
                }
                chunks.append(self._create_chunk(table_text, metadata, "table"))
                
        except Exception as e:
            raise Exception(f"DOCXå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_doc(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†DOCæ–‡ä»¶"""
        # å¯¹äºDOCæ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨python-docx2txtæˆ–å…¶ä»–åº“
        try:
            import docx2txt
            text = docx2txt.process(file_path)
            metadata = {
                "source": file_path,
                "type": "text"
            }
            return [self._create_chunk(text, metadata, "text")]
        except ImportError:
            raise ImportError("docx2txtæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†DOCæ–‡ä»¶")
    
    def _process_txt(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†TXTæ–‡ä»¶"""
        chunks = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = content.split('\n\n')
            for i, paragraph in enumerate(paragraphs):
                if paragraph.strip():
                    metadata = {
                        "paragraph": i + 1,
                        "source": file_path,
                        "type": "text"
                    }
                    chunks.append(self._create_chunk(paragraph, metadata, "text"))
                    
        except Exception as e:
            raise Exception(f"TXTå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_markdown(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†Markdownæ–‡ä»¶"""
        chunks = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„Markdownè§£æ
            lines = content.split('\n')
            current_section = []
            section_level = 0
            
            for line in lines:
                if line.startswith('#'):
                    # ä¿å­˜å½“å‰æ®µè½
                    if current_section:
                        section_text = '\n'.join(current_section)
                        metadata = {
                            "section_level": section_level,
                            "source": file_path,
                            "type": "text"
                        }
                        chunks.append(self._create_chunk(section_text, metadata, "text"))
                        current_section = []
                    
                    # å¼€å§‹æ–°æ®µè½
                    section_level = len(line) - len(line.lstrip('#'))
                    current_section = [line]
                else:
                    current_section.append(line)
            
            # ä¿å­˜æœ€åä¸€ä¸ªæ®µè½
            if current_section:
                section_text = '\n'.join(current_section)
                metadata = {
                    "section_level": section_level,
                    "source": file_path,
                    "type": "text"
                }
                chunks.append(self._create_chunk(section_text, metadata, "text"))
                
        except Exception as e:
            raise Exception(f"Markdownå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_csv(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†CSVæ–‡ä»¶"""
        if not PANDAS_AVAILABLE:
            raise ImportError("pandasæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†CSVæ–‡ä»¶")
        
        chunks = []
        try:
            df = pd.read_csv(file_path)
            
            # å°†æ•´ä¸ªCSVä½œä¸ºä¸€ä¸ªè¡¨æ ¼å—
            csv_text = df.to_string(index=False)
            metadata = {
                "rows": len(df),
                "columns": len(df.columns),
                "source": file_path,
                "type": "table"
            }
            chunks.append(self._create_chunk(csv_text, metadata, "table"))
            
            # å¦‚æœæ•°æ®é‡å¤§ï¼Œå¯ä»¥æŒ‰è¡Œåˆ†å‰²
            if len(df) > 100:
                for i in range(0, len(df), 50):
                    chunk_df = df.iloc[i:i+50]
                    chunk_text = chunk_df.to_string(index=False)
                    metadata = {
                        "rows": len(chunk_df),
                        "columns": len(chunk_df.columns),
                        "chunk_start": i,
                        "source": file_path,
                        "type": "table"
                    }
                    chunks.append(self._create_chunk(chunk_text, metadata, "table"))
                    
        except Exception as e:
            raise Exception(f"CSVå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_excel(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†Excelæ–‡ä»¶"""
        if not PANDAS_AVAILABLE or not EXCEL_AVAILABLE:
            raise ImportError("pandasæˆ–openpyxlæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†Excelæ–‡ä»¶")
        
        chunks = []
        try:
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_file = pd.ExcelFile(file_path)
            
            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                if not df.empty:
                    sheet_text = df.to_string(index=False)
                    metadata = {
                        "sheet": sheet_name,
                        "rows": len(df),
                        "columns": len(df.columns),
                        "source": file_path,
                        "type": "table"
                    }
                    chunks.append(self._create_chunk(sheet_text, metadata, "table"))
                    
        except Exception as e:
            raise Exception(f"Excelå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_pptx(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†PPTXæ–‡ä»¶"""
        if not PPTX_AVAILABLE:
            raise ImportError("python-pptxæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†PPTXæ–‡ä»¶")
        
        chunks = []
        try:
            prs = Presentation(file_path)
            
            for slide_index, slide in enumerate(prs.slides):
                slide_text = []
                
                # æå–æ–‡æœ¬
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        slide_text.append(shape.text)
                
                if slide_text:
                    text_content = '\n'.join(slide_text)
                    metadata = {
                        "slide": slide_index + 1,
                        "source": file_path,
                        "type": "text"
                    }
                    chunks.append(self._create_chunk(text_content, metadata, "text"))
                    
        except Exception as e:
            raise Exception(f"PPTXå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_ppt(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†PPTæ–‡ä»¶"""
        # PPTæ–‡ä»¶å¤„ç†è¾ƒå¤æ‚ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨win32comæˆ–å…¶ä»–åº“
        raise NotImplementedError("PPTæ ¼å¼å¤„ç†å°šæœªå®ç°")
    
    def _process_json(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†JSONæ–‡ä»¶"""
        chunks = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å°†JSONè½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬
            json_text = json.dumps(data, ensure_ascii=False, indent=2)
            metadata = {
                "source": file_path,
                "type": "data"
            }
            chunks.append(self._create_chunk(json_text, metadata, "text"))
            
        except Exception as e:
            raise Exception(f"JSONå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_xml(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†XMLæ–‡ä»¶"""
        chunks = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            metadata = {
                "source": file_path,
                "type": "data"
            }
            chunks.append(self._create_chunk(content, metadata, "text"))
            
        except Exception as e:
            raise Exception(f"XMLå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_html(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†HTMLæ–‡ä»¶"""
        chunks = []
        try:
            from bs4 import BeautifulSoup
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            text = soup.get_text()
            
            metadata = {
                "source": file_path,
                "type": "text"
            }
            chunks.append(self._create_chunk(text, metadata, "text"))
            
        except ImportError:
            raise ImportError("BeautifulSoup4æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†HTMLæ–‡ä»¶")
        except Exception as e:
            raise Exception(f"HTMLå¤„ç†å¤±è´¥: {e}")
        
        return chunks
    
    def _process_rtf(self, file_path: str) -> List[DocumentChunk]:
        """å¤„ç†RTFæ–‡ä»¶"""
        try:
            from striprtf.striprtf import rtf_to_text
            
            with open(file_path, 'r', encoding='utf-8') as f:
                rtf_content = f.read()
            
            text = rtf_to_text(rtf_content)
            metadata = {
                "source": file_path,
                "type": "text"
            }
            return [self._create_chunk(text, metadata, "text")]
            
        except ImportError:
            raise ImportError("striprtfæœªå®‰è£…ï¼Œæ— æ³•å¤„ç†RTFæ–‡ä»¶")
        except Exception as e:
            raise Exception(f"RTFå¤„ç†å¤±è´¥: {e}")
    
    def batch_process_documents(self, file_paths: List[str], 
                               max_workers: int = 4) -> Dict[str, List[DocumentChunk]]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_path = {
                executor.submit(self.process_document, path): path 
                for path in file_paths
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_path):
                file_path = future_to_path[future]
                try:
                    chunks = future.result()
                    results[file_path] = chunks
                    print(f"âœ… å®Œæˆå¤„ç†: {file_path}")
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥: {file_path} - {e}")
                    results[file_path] = []
        
        return results
    
    def get_document_info(self, file_path: str) -> Optional[DocumentMetadata]:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        return self.document_index.get(file_path)
    
    def list_cached_documents(self) -> List[DocumentMetadata]:
        """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„æ–‡æ¡£"""
        return list(self.document_index.values())
    
    def clear_cache(self, file_path: Optional[str] = None):
        """æ¸…ç†ç¼“å­˜"""
        if file_path:
            # æ¸…ç†ç‰¹å®šæ–‡æ¡£çš„ç¼“å­˜
            if file_path in self.document_index:
                metadata = self.document_index[file_path]
                cache_file = self.cache_dir / "chunks" / f"{metadata.file_hash}.pkl"
                if cache_file.exists():
                    cache_file.unlink()
                del self.document_index[file_path]
                self._save_document_index()
                print(f"ğŸ—‘ï¸ å·²æ¸…ç†ç¼“å­˜: {file_path}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            import shutil
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(exist_ok=True)
                (self.cache_dir / "metadata").mkdir(exist_ok=True)
                (self.cache_dir / "chunks").mkdir(exist_ok=True)
                (self.cache_dir / "embeddings").mkdir(exist_ok=True)
            self.document_index = {}
            print("ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰ç¼“å­˜")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_size = 0
        file_count = 0
        
        if self.cache_dir.exists():
            for cache_file in (self.cache_dir / "chunks").glob("*.pkl"):
                total_size += cache_file.stat().st_size
                file_count += 1
        
        return {
            "cached_documents": len(self.document_index),
            "cache_files": file_count,
            "total_size_mb": total_size / (1024 * 1024),
            "max_size_mb": self.max_cache_size_mb,
            "cache_usage_percent": (total_size / (1024 * 1024)) / self.max_cache_size_mb * 100
        }

def main():
    """æµ‹è¯•å‡½æ•°"""
    manager = EnhancedDocumentManager()
    
    # æµ‹è¯•å¤„ç†æ–‡æ¡£
    test_file = "test.pdf"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    if os.path.exists(test_file):
        chunks = manager.process_document(test_file)
        print(f"å¤„ç†ç»“æœ: {len(chunks)} ä¸ªå—")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        stats = manager.get_cache_stats()
        print(f"ç¼“å­˜ç»Ÿè®¡: {stats}")

if __name__ == "__main__":
    main()