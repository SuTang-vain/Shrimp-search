"""
å¢å¼ºçš„LLMæ¥å£ V2 - æ”¯æŒå¤šç«¯æ¨¡å‹é€‰æ‹©
é›†æˆModelScopeè¿œç¨‹APIå’ŒOllamaæœ¬åœ°éƒ¨ç½²
"""

import os
import time
import requests
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass

from model_manager import ModelManager, ModelConfig
from ollama_interface import OllamaInterface

class ContentFilterException(Exception):
    """å†…å®¹è¿‡æ»¤å¼‚å¸¸"""
    pass

@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœæ•°æ®ç±»"""
    content: str
    model_key: str
    model_type: str
    generation_time: float
    token_count: Optional[int] = None
    error: Optional[str] = None

class EnhancedLLMInterfaceV2:
    """å¢å¼ºçš„LLMæ¥å£V2 - ç»Ÿä¸€ç®¡ç†å¤šç§æ¨¡å‹åç«¯"""
    
    def __init__(self, api_key: Optional[str] = None):
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        from dotenv import load_dotenv
        load_dotenv()
        
        self.api_key = api_key or os.getenv('MODELSCOPE_SDK_TOKEN')
        self.glm_api_key = os.getenv('GLM_API_KEY')
        self.model_manager = ModelManager()
        self.ollama_interface = OllamaInterface()
        
        # å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
        self.current_backend = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.generation_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_time': 0.0,
            'model_usage': {},
            'content_filter_errors': 0,
            'auto_fallbacks': 0
        }
        
        # æ•æ„Ÿè¯æ›¿æ¢æ˜ å°„
        self.sensitive_word_replacements = {
            'camel': 'CAMELæ¡†æ¶',
            'CAMEL': 'CAMELæ¡†æ¶',
            'rag': 'RAGæŠ€æœ¯',
            'RAG': 'RAGæŠ€æœ¯',
            'ç³»ç»Ÿ': 'æŠ€æœ¯æ¶æ„',
            'å®ç°': 'æŠ€æœ¯å®ç°æ–¹æ¡ˆ'
        }
        
        print("å¢å¼ºLLMæ¥å£V2åˆå§‹åŒ–å®Œæˆ")
        self._print_available_models()
    
    def _print_available_models(self):
        """æ‰“å°å¯ç”¨æ¨¡å‹ä¿¡æ¯"""
        print("\nå¯ç”¨æ¨¡å‹:")
        
        # è¿œç¨‹æ¨¡å‹
        remote_models = self.model_manager.list_models_by_type('remote')
        if remote_models:
            print("  è¿œç¨‹æ¨¡å‹ (ModelScope):")
            for key, config in remote_models.items():
                print(f"    - {key}: {config.description}")
        
        # æœ¬åœ°æ¨¡å‹
        local_models = self.model_manager.list_models_by_type('local')
        if local_models:
            print("  æœ¬åœ°æ¨¡å‹ (Ollama):")
            ollama_status = "å¯ç”¨" if self.ollama_interface.is_available() else "ä¸å¯ç”¨"
            print(f"    OllamaæœåŠ¡çŠ¶æ€: {ollama_status}")
            
            if self.ollama_interface.is_available():
                installed_models = self.ollama_interface.get_installed_models()
                for key, config in local_models.items():
                    model_installed = any(m['name'] == config.name for m in installed_models)
                    status = "å·²å®‰è£…" if model_installed else "æœªå®‰è£…"
                    print(f"    - {key}: {config.description} ({status})")
            else:
                for key, config in local_models.items():
                    print(f"    - {key}: {config.description} (æœåŠ¡ä¸å¯ç”¨)")
    
    def set_model(self, model_key: str) -> bool:
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹"""
        result = self.model_manager.get_model_config(model_key)
        if not result:
            print(f"æœªæ‰¾åˆ°æ¨¡å‹: {model_key}")
            return False
        
        model_type, model_config = result
        
        # éªŒè¯æ¨¡å‹å¯ç”¨æ€§
        if model_type == 'remote':
            if model_config.provider == 'GLM':
                if not self.glm_api_key:
                    print("ä½¿ç”¨GLMæ¨¡å‹éœ€è¦GLM_API_KEY")
                    return False
            else:
                if not self.api_key:
                    print("ä½¿ç”¨ModelScopeæ¨¡å‹éœ€è¦MODELSCOPE_SDK_TOKEN")
                    return False
        elif model_type == 'local':
            if not self.ollama_interface.is_available():
                print("OllamaæœåŠ¡ä¸å¯ç”¨")
                return False
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å®‰è£…
            if not self.ollama_interface.check_model_exists(model_config.name):
                print(f"æœ¬åœ°æ¨¡å‹ {model_config.name} æœªå®‰è£…ï¼Œå°è¯•è‡ªåŠ¨æ‹‰å–...")
                if not self.ollama_interface.pull_model(model_config.name):
                    print(f"æ¨¡å‹ {model_config.name} æ‹‰å–å¤±è´¥")
                    return False
        
        # è®¾ç½®å½“å‰åç«¯
        self.current_backend = {
            'type': model_type,
            'key': model_key,
            'config': model_config
        }
        
        # æ›´æ–°æ¨¡å‹ç®¡ç†å™¨
        self.model_manager.set_current_model(model_key)
        
        print(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_key} ({model_config.provider})")
        return True
    
    def get_current_model(self) -> Optional[Dict[str, Any]]:
        """è·å–å½“å‰æ¨¡å‹ä¿¡æ¯"""
        if self.current_backend:
            return {
                'key': self.current_backend['key'],
                'type': self.current_backend['type'],
                'name': self.current_backend['config'].name,
                'provider': self.current_backend['config'].provider,
                'description': self.current_backend['config'].description
            }
        return None
    
    def _optimize_query_for_content_filter(self, text: str) -> str:
        """ä¼˜åŒ–æŸ¥è¯¢æ–‡æœ¬ä»¥é¿å…å†…å®¹è¿‡æ»¤"""
        optimized_text = text
        
        # æ›¿æ¢å¯èƒ½çš„æ•æ„Ÿè¯
        for sensitive_word, replacement in self.sensitive_word_replacements.items():
            optimized_text = optimized_text.replace(sensitive_word, replacement)
        
        # æ·»åŠ æŠ€æœ¯æ€§å‰ç¼€ï¼Œä½¿æŸ¥è¯¢æ›´åŠ å­¦æœ¯åŒ–
        if any(word in text.lower() for word in ['camel', 'rag', 'ç³»ç»Ÿ', 'å®ç°']):
            optimized_text = f"è¯·ä»æŠ€æœ¯è§’åº¦åˆ†æï¼š{optimized_text}"
        
        return optimized_text

    def _try_fallback_model(self, original_prompt: str, max_tokens: int, temperature: float, **kwargs) -> str:
        """å°è¯•å¤‡ç”¨æ¨¡å‹"""
        print("ğŸ”„ å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹...")
        
        # ä¿å­˜å½“å‰æ¨¡å‹
        original_backend = self.current_backend
        
        # å°è¯•åˆ‡æ¢åˆ°ModelScopeæ¨¡å‹
        available_models = self.list_available_models()
        fallback_models = []
        
        # ä¼˜å…ˆé€‰æ‹©ModelScopeæ¨¡å‹ä½œä¸ºå¤‡ç”¨
        for key, info in available_models['remote'].items():
            if info['available'] and info['provider'] != 'GLM' and key != original_backend['key']:
                fallback_models.append(key)
        
        # ç„¶åé€‰æ‹©æœ¬åœ°æ¨¡å‹
        for key, info in available_models['local'].items():
            if info['available'] and info['installed'] and key != original_backend['key']:
                fallback_models.append(key)
        
        for fallback_key in fallback_models:
            try:
                print(f"ğŸ”„ å°è¯•åˆ‡æ¢åˆ°æ¨¡å‹: {fallback_key}")
                if self.set_model(fallback_key):
                    # ä½¿ç”¨ä¼˜åŒ–åçš„æŸ¥è¯¢
                    optimized_prompt = self._optimize_query_for_content_filter(original_prompt)
                    
                    if self.current_backend['type'] == 'local':
                        result = self._generate_local(optimized_prompt, max_tokens, temperature, **kwargs)
                    else:
                        result = self._generate_remote(optimized_prompt, max_tokens, temperature, **kwargs)
                    
                    self.generation_stats['auto_fallbacks'] += 1
                    print(f"âœ… å¤‡ç”¨æ¨¡å‹ {fallback_key} ç”ŸæˆæˆåŠŸ")
                    return result
                    
            except Exception as e:
                print(f"âŒ å¤‡ç”¨æ¨¡å‹ {fallback_key} ä¹Ÿå¤±è´¥: {str(e)}")
                continue
        
        # æ¢å¤åŸæ¨¡å‹
        self.current_backend = original_backend
        raise Exception("æ‰€æœ‰å¤‡ç”¨æ¨¡å‹éƒ½å¤±è´¥äº†")

    def generate(self, prompt: str, max_tokens: Optional[int] = None, 
                temperature: Optional[float] = None, **kwargs) -> GenerationResult:
        """ç»Ÿä¸€çš„æ–‡æœ¬ç”Ÿæˆæ¥å£"""
        if not self.current_backend:
            return GenerationResult(
                content="",
                model_key="",
                model_type="",
                generation_time=0.0,
                error="æœªè®¾ç½®æ¨¡å‹ï¼Œè¯·å…ˆé€‰æ‹©æ¨¡å‹"
            )
        
        # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼æˆ–ä¼ å…¥çš„å‚æ•°
        config = self.current_backend['config']
        max_tokens = max_tokens or config.max_tokens
        temperature = temperature or config.temperature
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ›´æ–°ç»Ÿè®¡
        self.generation_stats['total_requests'] += 1
        model_key = self.current_backend['key']
        if model_key not in self.generation_stats['model_usage']:
            self.generation_stats['model_usage'][model_key] = 0
        self.generation_stats['model_usage'][model_key] += 1
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç”Ÿæˆæ–¹æ³•
            if self.current_backend['type'] == 'local':
                content = self._generate_local(prompt, max_tokens, temperature, **kwargs)
            else:
                content = self._generate_remote(prompt, max_tokens, temperature, **kwargs)
            
            # è®¡ç®—ç”Ÿæˆæ—¶é—´
            generation_time = time.time() - start_time
            self.generation_stats['successful_requests'] += 1
            self.generation_stats['total_time'] += generation_time
            
            return GenerationResult(
                content=content,
                model_key=model_key,
                model_type=self.current_backend['type'],
                generation_time=generation_time
            )
            
        except ContentFilterException as e:
            print(f"âš ï¸ å†…å®¹è¿‡æ»¤é”™è¯¯: {str(e)}")
            self.generation_stats['content_filter_errors'] += 1
            
            try:
                # å°è¯•å¤‡ç”¨æ¨¡å‹
                content = self._try_fallback_model(prompt, max_tokens, temperature, **kwargs)
                generation_time = time.time() - start_time
                self.generation_stats['successful_requests'] += 1
                self.generation_stats['total_time'] += generation_time
                
                return GenerationResult(
                    content=content,
                    model_key=self.current_backend['key'],
                    model_type=self.current_backend['type'],
                    generation_time=generation_time
                )
                
            except Exception as fallback_error:
                generation_time = time.time() - start_time
                self.generation_stats['failed_requests'] += 1
                
                error_msg = f"å†…å®¹è¿‡æ»¤é”™è¯¯ï¼Œå¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥: {str(fallback_error)}"
                print(f"âŒ {error_msg}")
                
                return GenerationResult(
                    content="",
                    model_key=model_key,
                    model_type=self.current_backend['type'],
                    generation_time=generation_time,
                    error=error_msg
                )
            
        except Exception as e:
            generation_time = time.time() - start_time
            self.generation_stats['failed_requests'] += 1
            
            error_msg = f"ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"æ¨¡å‹ç”Ÿæˆé”™è¯¯: {error_msg}")
            
            return GenerationResult(
                content="",
                model_key=model_key,
                model_type=self.current_backend['type'],
                generation_time=generation_time,
                error=error_msg
            )
    
    def _generate_local(self, prompt: str, max_tokens: int, temperature: float, **kwargs) -> str:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        model_name = self.current_backend['config'].name
        
        # æ”¯æŒèŠå¤©æ¨¡å¼
        if 'messages' in kwargs:
            return self.ollama_interface.generate_chat(
                model_name=model_name,
                messages=kwargs['messages'],
                max_tokens=max_tokens,
                temperature=temperature
            )
        else:
            return self.ollama_interface.generate(
                model_name=model_name,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=kwargs.get('stream', False)
            )
    
    def _generate_remote(self, prompt: str, max_tokens: int, temperature: float, **kwargs) -> str:
        """è¿œç¨‹æ¨¡å‹ç”Ÿæˆ"""
        config = self.current_backend['config']
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©APIå¯†é’¥å’Œè¯·æ±‚æ ¼å¼
        if config.provider == 'GLM':
            if not self.glm_api_key:
                raise Exception("GLMæ¨¡å‹éœ€è¦GLM_API_KEY")
            api_key = self.glm_api_key
        else:
            if not self.api_key:
                raise Exception("ModelScopeæ¨¡å‹éœ€è¦MODELSCOPE_SDK_TOKEN")
            api_key = self.api_key
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # æ”¯æŒèŠå¤©æ ¼å¼
        if 'messages' in kwargs:
            messages = kwargs['messages']
        else:
            messages = [{"role": "user", "content": prompt}]
        
        data = {
            "model": config.name,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        # é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    config.api_url,
                    headers=headers,
                    json=data,
                    timeout=config.timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if 'choices' in result and len(result['choices']) > 0:
                        return result['choices'][0]['message']['content']
                    else:
                        raise Exception(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                else:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹è¿‡æ»¤é”™è¯¯
                    if response.status_code == 400:
                        try:
                            error_data = response.json()
                            if 'error' in error_data and 'code' in error_data['error']:
                                error_code = error_data['error']['code']
                                if error_code == '1301':  # GLMå†…å®¹è¿‡æ»¤é”™è¯¯
                                    raise ContentFilterException(f"å†…å®¹è¿‡æ»¤é”™è¯¯: {error_data['error']['message']}")
                        except (ValueError, KeyError):
                            pass
                    
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥ (çŠ¶æ€ç : {response.status_code}): {response.text}")
                    
            except ContentFilterException:
                # å†…å®¹è¿‡æ»¤é”™è¯¯ä¸é‡è¯•ï¼Œç›´æ¥æŠ›å‡º
                raise
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    print(f"è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    raise Exception("è¯·æ±‚è¶…æ—¶")
            except requests.exceptions.ConnectionError:
                if attempt < max_retries - 1:
                    print(f"è¿æ¥é”™è¯¯ï¼Œé‡è¯•ä¸­... ({attempt + 1}/{max_retries})")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    raise Exception("è¿æ¥é”™è¯¯")
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­... ({attempt + 1}/{max_retries}): {str(e)}")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    raise e
        
        raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
    
    def generate_chat(self, messages: list, max_tokens: Optional[int] = None,
                     temperature: Optional[float] = None) -> GenerationResult:
        """èŠå¤©æ¨¡å¼ç”Ÿæˆ"""
        return self.generate("", max_tokens, temperature, messages=messages)
    
    def list_available_models(self) -> Dict[str, Any]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        result = {
            'remote': {},
            'local': {}
        }
        
        # è¿œç¨‹æ¨¡å‹
        remote_models = self.model_manager.list_models_by_type('remote')
        for key, config in remote_models.items():
            # æ ¹æ®æä¾›å•†æ£€æŸ¥å¯¹åº”çš„APIå¯†é’¥
            if config.provider == 'GLM':
                available = bool(self.glm_api_key)
            else:
                available = bool(self.api_key)
            
            result['remote'][key] = {
                'name': config.name,
                'provider': config.provider,
                'description': config.description,
                'available': available
            }
        
        # æœ¬åœ°æ¨¡å‹
        local_models = self.model_manager.list_models_by_type('local')
        ollama_available = self.ollama_interface.is_available()
        installed_models = []
        
        if ollama_available:
            installed_models = [m['name'] for m in self.ollama_interface.get_installed_models()]
        
        for key, config in local_models.items():
            result['local'][key] = {
                'name': config.name,
                'provider': config.provider,
                'description': config.description,
                'available': ollama_available,
                'installed': config.name in installed_models
            }
        
        return result
    
    def switch_model(self, model_key: str) -> bool:
        """åˆ‡æ¢æ¨¡å‹ï¼ˆset_modelçš„åˆ«åï¼‰"""
        return self.set_model(model_key)
    
    def get_generation_stats(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = self.generation_stats.copy()
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if stats['successful_requests'] > 0:
            stats['average_time'] = stats['total_time'] / stats['successful_requests']
        else:
            stats['average_time'] = 0.0
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_requests'] > 0:
            stats['success_rate'] = stats['successful_requests'] / stats['total_requests']
        else:
            stats['success_rate'] = 0.0
        
        return stats
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.generation_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_time': 0.0,
            'model_usage': {}
        }
        print("ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ¥å£çŠ¶æ€"""
        current_model = self.get_current_model()
        stats = self.get_generation_stats()
        
        return {
            'current_model': current_model,
            'api_key_available': bool(self.api_key),
            'ollama_available': self.ollama_interface.is_available(),
            'ollama_models': len(self.ollama_interface.get_installed_models()),
            'generation_stats': stats,
            'model_manager_stats': self.model_manager.get_model_statistics()
        }
    
    def test_model(self, model_key: Optional[str] = None) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        if model_key:
            # ä¸´æ—¶åˆ‡æ¢åˆ°æŒ‡å®šæ¨¡å‹è¿›è¡Œæµ‹è¯•
            original_backend = self.current_backend
            if not self.set_model(model_key):
                return {
                    'success': False,
                    'error': f'æ— æ³•åˆ‡æ¢åˆ°æ¨¡å‹: {model_key}',
                    'model_key': model_key
                }
        
        if not self.current_backend:
            return {
                'success': False,
                'error': 'æœªè®¾ç½®æ¨¡å‹',
                'model_key': None
            }
        
        test_prompt = "è¯·å›å¤'æµ‹è¯•æˆåŠŸ'"
        
        try:
            result = self.generate(test_prompt, max_tokens=50, temperature=0.1)
            
            success = not result.error and len(result.content.strip()) > 0
            
            test_result = {
                'success': success,
                'model_key': result.model_key,
                'model_type': result.model_type,
                'response': result.content,
                'generation_time': result.generation_time,
                'error': result.error
            }
            
            # å¦‚æœæ˜¯ä¸´æ—¶æµ‹è¯•ï¼Œæ¢å¤åŸæ¥çš„æ¨¡å‹
            if model_key and original_backend:
                self.current_backend = original_backend
            
            return test_result
            
        except Exception as e:
            # å¦‚æœæ˜¯ä¸´æ—¶æµ‹è¯•ï¼Œæ¢å¤åŸæ¥çš„æ¨¡å‹
            if model_key and original_backend:
                self.current_backend = original_backend
            
            return {
                'success': False,
                'error': str(e),
                'model_key': self.current_backend['key'] if self.current_backend else None
            }
    
    def auto_select_best_model(self) -> Optional[str]:
        """è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹"""
        available_models = self.list_available_models()
        
        # ä¼˜å…ˆçº§ï¼šæœ¬åœ°å·²å®‰è£…æ¨¡å‹ > è¿œç¨‹æ¨¡å‹ > æœ¬åœ°æœªå®‰è£…æ¨¡å‹
        
        # 1. æ£€æŸ¥æœ¬åœ°å·²å®‰è£…æ¨¡å‹
        for key, info in available_models['local'].items():
            if info['available'] and info['installed']:
                if self.set_model(key):
                    print(f"è‡ªåŠ¨é€‰æ‹©æœ¬åœ°æ¨¡å‹: {key}")
                    return key
        
        # 2. æ£€æŸ¥è¿œç¨‹æ¨¡å‹
        if self.api_key:
            for key, info in available_models['remote'].items():
                if info['available']:
                    if self.set_model(key):
                        print(f"è‡ªåŠ¨é€‰æ‹©è¿œç¨‹æ¨¡å‹: {key}")
                        return key
        
        # 3. å°è¯•æœ¬åœ°æœªå®‰è£…æ¨¡å‹ï¼ˆä¼šè‡ªåŠ¨æ‹‰å–ï¼‰
        for key, info in available_models['local'].items():
            if info['available'] and not info['installed']:
                if self.set_model(key):
                    print(f"è‡ªåŠ¨é€‰æ‹©å¹¶å®‰è£…æœ¬åœ°æ¨¡å‹: {key}")
                    return key
        
        print("æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹")
        return None