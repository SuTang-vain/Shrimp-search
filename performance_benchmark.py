#!/usr/bin/env python3
"""
ç¡¬ä»¶èµ„æºå ç”¨æŠ¥å‘Šç”Ÿæˆè„šæœ¬
ç›‘æ§å’ŒæŠ¥å‘ŠRAGç³»ç»Ÿçš„ç¡¬ä»¶èµ„æºä½¿ç”¨æƒ…å†µ
"""

import os
import sys
import time
import json
import psutil
import threading
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import statistics
from pathlib import Path

# å°è¯•å¯¼å…¥GPUç›‘æ§åº“
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    print("è­¦å‘Š: GPUtilæœªå®‰è£…ï¼ŒGPUç›‘æ§åŠŸèƒ½ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥CUDAç›¸å…³åº“
try:
    import torch
    TORCH_AVAILABLE = torch.cuda.is_available()
except ImportError:
    TORCH_AVAILABLE = False

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from performance_monitor import PerformanceMonitor, PerformanceContext
    from Enhanced_Interactive_Multimodal_RAG_v2 import EnhancedRAGSystemV2
    from enhanced_document_manager import EnhancedDocumentManager
except ImportError as e:
    print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é¡¹ç›®æ¨¡å—: {e}")
    sys.exit(1)

@dataclass
class SystemResourceMetrics:
    """ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_used_gb: float
    memory_available_gb: float
    disk_usage_percent: float
    gpu_metrics: List[Dict[str, Any]] = field(default_factory=list)
    network_io: Dict[str, int] = field(default_factory=dict)
    process_metrics: Dict[str, Any] = field(default_factory=dict)

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    duration: float
    success: bool
    throughput: Optional[float] = None
    latency_stats: Optional[Dict[str, float]] = None
    resource_usage: Optional[SystemResourceMetrics] = None
    error_message: str = ""
    additional_metrics: Dict[str, Any] = field(default_factory=dict)

class HardwareResourceMonitor:
    """ç¡¬ä»¶èµ„æºç›‘æ§å™¨"""
    
    def __init__(self, monitoring_interval: float = 1.0):
        self.monitoring_interval = monitoring_interval
        self.is_monitoring = False
        self.resource_history: List[SystemResourceMetrics] = []
        self.monitor_thread: Optional[threading.Thread] = None
        self.lock = threading.Lock()
        
        # è·å–å½“å‰è¿›ç¨‹
        self.current_process = psutil.Process()
        
        print(f"ç¡¬ä»¶èµ„æºç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ (ç›‘æ§é—´éš”: {monitoring_interval}s)")
        
    def get_gpu_metrics(self) -> List[Dict[str, Any]]:
        """è·å–GPUæŒ‡æ ‡"""
        gpu_metrics = []
        
        if GPU_AVAILABLE:
            try:
                gpus = GPUtil.getGPUs()
                for i, gpu in enumerate(gpus):
                    gpu_info = {
                        'id': i,
                        'name': gpu.name,
                        'load': gpu.load * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        'memory_used': gpu.memoryUsed,  # MB
                        'memory_total': gpu.memoryTotal,  # MB
                        'memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,
                        'temperature': gpu.temperature
                    }
                    gpu_metrics.append(gpu_info)
            except Exception as e:
                print(f"è·å–GPUæŒ‡æ ‡å¤±è´¥: {e}")
        
        # å¦‚æœæœ‰PyTorch CUDAæ”¯æŒï¼Œæ·»åŠ CUDAä¿¡æ¯
        if TORCH_AVAILABLE:
            try:
                for i in range(torch.cuda.device_count()):
                    device = torch.cuda.device(i)
                    memory_allocated = torch.cuda.memory_allocated(i) / 1024**2  # MB
                    memory_cached = torch.cuda.memory_reserved(i) / 1024**2  # MB
                    
                    # æ›´æ–°å¯¹åº”GPUçš„ä¿¡æ¯
                    if i < len(gpu_metrics):
                        gpu_metrics[i].update({
                            'cuda_memory_allocated': memory_allocated,
                            'cuda_memory_cached': memory_cached
                        })
                    else:
                        # å¦‚æœGPUtilä¸å¯ç”¨ï¼Œåˆ›å»ºåŸºæœ¬çš„CUDAä¿¡æ¯
                        gpu_metrics.append({
                            'id': i,
                            'name': torch.cuda.get_device_name(i),
                            'cuda_memory_allocated': memory_allocated,
                            'cuda_memory_cached': memory_cached
                        })
            except Exception as e:
                print(f"è·å–CUDAä¿¡æ¯å¤±è´¥: {e}")
        
        return gpu_metrics
    
    def get_current_metrics(self) -> SystemResourceMetrics:
        """è·å–å½“å‰ç³»ç»Ÿèµ„æºæŒ‡æ ‡"""
        # CPUå’Œå†…å­˜ä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        # ç½‘ç»œIO
        net_io = psutil.net_io_counters()
        network_io = {
            'bytes_sent': net_io.bytes_sent,
            'bytes_recv': net_io.bytes_recv,
            'packets_sent': net_io.packets_sent,
            'packets_recv': net_io.packets_recv
        }
        
        # å½“å‰è¿›ç¨‹ä¿¡æ¯
        try:
            process_info = {
                'cpu_percent': self.current_process.cpu_percent(),
                'memory_percent': self.current_process.memory_percent(),
                'memory_info': self.current_process.memory_info()._asdict(),
                'num_threads': self.current_process.num_threads(),
                'open_files': len(self.current_process.open_files())
            }
        except Exception as e:
            process_info = {'error': str(e)}
        
        return SystemResourceMetrics(
            timestamp=time.time(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_used_gb=memory.used / (1024**3),
            memory_available_gb=memory.available / (1024**3),
            disk_usage_percent=disk.percent,
            gpu_metrics=self.get_gpu_metrics(),
            network_io=network_io,
            process_metrics=process_info
        )
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        print("å¼€å§‹ç¡¬ä»¶èµ„æºç›‘æ§...")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
        print("ç¡¬ä»¶èµ„æºç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.is_monitoring:
            try:
                metrics = self.get_current_metrics()
                with self.lock:
                    self.resource_history.append(metrics)
                    # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                    if len(self.resource_history) > 1000:
                        self.resource_history = self.resource_history[-500:]
                
                time.sleep(self.monitoring_interval)
            except Exception as e:
                print(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(self.monitoring_interval)
    
    def get_resource_summary(self) -> Dict[str, Any]:
        """è·å–èµ„æºä½¿ç”¨æ‘˜è¦"""
        if not self.resource_history:
            return {"error": "æ— ç›‘æ§æ•°æ®"}
        
        with self.lock:
            history = self.resource_history.copy()
        
        # CPUç»Ÿè®¡
        cpu_values = [m.cpu_percent for m in history]
        cpu_stats = {
            'min': min(cpu_values),
            'max': max(cpu_values),
            'avg': statistics.mean(cpu_values),
            'median': statistics.median(cpu_values)
        }
        
        # å†…å­˜ç»Ÿè®¡
        memory_values = [m.memory_percent for m in history]
        memory_stats = {
            'min': min(memory_values),
            'max': max(memory_values),
            'avg': statistics.mean(memory_values),
            'median': statistics.median(memory_values)
        }
        
        # GPUç»Ÿè®¡
        gpu_stats = {}
        if history and history[0].gpu_metrics:
            for gpu_id in range(len(history[0].gpu_metrics)):
                gpu_loads = []
                gpu_memory_percents = []
                
                for metrics in history:
                    if gpu_id < len(metrics.gpu_metrics):
                        gpu_info = metrics.gpu_metrics[gpu_id]
                        if 'load' in gpu_info:
                            gpu_loads.append(gpu_info['load'])
                        if 'memory_percent' in gpu_info:
                            gpu_memory_percents.append(gpu_info['memory_percent'])
                
                if gpu_loads:
                    gpu_stats[f'gpu_{gpu_id}_load'] = {
                        'min': min(gpu_loads),
                        'max': max(gpu_loads),
                        'avg': statistics.mean(gpu_loads),
                        'median': statistics.median(gpu_loads)
                    }
                
                if gpu_memory_percents:
                    gpu_stats[f'gpu_{gpu_id}_memory'] = {
                        'min': min(gpu_memory_percents),
                        'max': max(gpu_memory_percents),
                        'avg': statistics.mean(gpu_memory_percents),
                        'median': statistics.median(gpu_memory_percents)
                    }
        
        return {
            'monitoring_period': {
                'start': datetime.fromtimestamp(history[0].timestamp).isoformat(),
                'end': datetime.fromtimestamp(history[-1].timestamp).isoformat(),
                'duration_seconds': history[-1].timestamp - history[0].timestamp,
                'sample_count': len(history)
            },
            'cpu_stats': cpu_stats,
            'memory_stats': memory_stats,
            'gpu_stats': gpu_stats,
            'current_metrics': self.get_current_metrics().__dict__
        }

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, cache_dir: str = "document_cache"):
        self.cache_dir = cache_dir
        self.resource_monitor = HardwareResourceMonitor()
        self.performance_monitor = PerformanceMonitor(enable_detailed_monitoring=True)
        self.benchmark_results: List[BenchmarkResult] = []
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        try:
            self.rag_system = EnhancedRAGSystemV2(
                enable_performance_monitoring=True,
                cache_dir=cache_dir
            )
            print("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_system = None
    
    def run_document_processing_benchmark(self, test_documents: List[str]) -> BenchmarkResult:
        """æ–‡æ¡£å¤„ç†æ€§èƒ½æµ‹è¯•"""
        print("\n=== æ–‡æ¡£å¤„ç†æ€§èƒ½æµ‹è¯• ===")
        
        if not self.rag_system:
            return BenchmarkResult(
                test_name="document_processing",
                duration=0,
                success=False,
                error_message="RAGç³»ç»Ÿæœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        self.resource_monitor.start_monitoring()
        
        try:
            # æ‰§è¡Œæ–‡æ¡£å¤„ç†
            self.rag_system.setup_knowledge_base(
                sources=test_documents,
                source_type="path",
                force_reprocess=True
            )
            
            duration = time.time() - start_time
            success = True
            
            # è®¡ç®—ååé‡
            throughput = len(test_documents) / duration if duration > 0 else 0
            
            print(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {len(test_documents)} ä¸ªæ–‡æ¡£, è€—æ—¶ {duration:.2f}s")
            print(f"å¤„ç†é€Ÿåº¦: {throughput:.2f} docs/s")
            
        except Exception as e:
            duration = time.time() - start_time
            success = False
            throughput = 0
            print(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        
        self.resource_monitor.stop_monitoring()
        
        result = BenchmarkResult(
            test_name="document_processing",
            duration=duration,
            success=success,
            throughput=throughput,
            resource_usage=self.resource_monitor.get_current_metrics(),
            additional_metrics={
                'document_count': len(test_documents),
                'resource_summary': self.resource_monitor.get_resource_summary()
            }
        )
        
        self.benchmark_results.append(result)
        return result
    
    def run_query_latency_benchmark(self, test_queries: List[str], iterations: int = 10) -> BenchmarkResult:
        """æŸ¥è¯¢å»¶è¿Ÿæ€§èƒ½æµ‹è¯•"""
        print(f"\n=== æŸ¥è¯¢å»¶è¿Ÿæ€§èƒ½æµ‹è¯• (æŸ¥è¯¢æ•°: {len(test_queries)}, è¿­ä»£: {iterations}) ===")
        
        if not self.rag_system or not self.rag_system.knowledge_base_initialized:
            return BenchmarkResult(
                test_name="query_latency",
                duration=0,
                success=False,
                error_message="çŸ¥è¯†åº“æœªåˆå§‹åŒ–"
            )
        
        latencies = []
        start_time = time.time()
        self.resource_monitor.start_monitoring()
        
        try:
            # ç¡®ä¿RAGç³»ç»Ÿå·²è®¾ç½®æ¨¡å‹
            if not hasattr(self.rag_system, 'llm') or self.rag_system.llm is None:
                print("æ­£åœ¨è®¾ç½®æŸ¥è¯¢æ¨¡å‹...")
                self.rag_system.setup_llm(
                    model_name="qwen2.5:7b",
                    base_url="http://localhost:11434",
                    temperature=0.7
                )
            
            for i in range(iterations):
                for query in test_queries:
                    query_start = time.time()
                    
                    # æ‰§è¡ŒæŸ¥è¯¢
                    result = self.rag_system.enhanced_query_v2(
                        query=query,
                        retrieval_mode="å¿«é€Ÿæ£€ç´¢"
                    )
                    
                    query_latency = time.time() - query_start
                    latencies.append(query_latency)
                    
                    print(f"æŸ¥è¯¢ {i+1}/{iterations}: {query[:50]}... -> {query_latency:.3f}s")
            
            duration = time.time() - start_time
            success = True
            
            # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
            latency_stats = {
                'min': min(latencies),
                'max': max(latencies),
                'avg': statistics.mean(latencies),
                'median': statistics.median(latencies),
                'p95': statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies),
                'p99': statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies)
            }
            
            throughput = len(latencies) / duration if duration > 0 else 0
            
            print(f"æŸ¥è¯¢æµ‹è¯•å®Œæˆ: {len(latencies)} æ¬¡æŸ¥è¯¢, æ€»è€—æ—¶ {duration:.2f}s")
            print(f"å¹³å‡å»¶è¿Ÿ: {latency_stats['avg']:.3f}s, æŸ¥è¯¢é€Ÿåº¦: {throughput:.2f} queries/s")
            
        except Exception as e:
            duration = time.time() - start_time
            success = False
            latency_stats = None
            throughput = 0
            print(f"æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        
        self.resource_monitor.stop_monitoring()
        
        result = BenchmarkResult(
            test_name="query_latency",
            duration=duration,
            success=success,
            throughput=throughput,
            latency_stats=latency_stats,
            resource_usage=self.resource_monitor.get_current_metrics(),
            additional_metrics={
                'query_count': len(test_queries),
                'iterations': iterations,
                'total_queries': len(latencies),
                'resource_summary': self.resource_monitor.get_resource_summary()
            }
        )
        
        self.benchmark_results.append(result)
        return result
    
    def run_vectorization_benchmark(self, test_texts: List[str]) -> BenchmarkResult:
        """å‘é‡åŒ–æ€§èƒ½æµ‹è¯•"""
        print(f"\n=== å‘é‡åŒ–æ€§èƒ½æµ‹è¯• (æ–‡æœ¬æ•°: {len(test_texts)}) ===")
        
        if not self.rag_system:
            return BenchmarkResult(
                test_name="vectorization",
                duration=0,
                success=False,
                error_message="RAGç³»ç»Ÿæœªåˆå§‹åŒ–"
            )
        
        start_time = time.time()
        self.resource_monitor.start_monitoring()
        
        try:
            # åˆ›å»ºæµ‹è¯•å…ƒæ•°æ®
            test_metadata = [{'source': f'test_{i}', 'chunk_id': i} for i in range(len(test_texts))]
            
            # æ‰§è¡Œå‘é‡åŒ–
            self.rag_system.vector_retriever.add_documents(
                texts=test_texts,
                metadata=test_metadata,
                batch_size=32,
                show_progress=True
            )
            
            duration = time.time() - start_time
            success = True
            throughput = len(test_texts) / duration if duration > 0 else 0
            
            print(f"å‘é‡åŒ–å®Œæˆ: {len(test_texts)} ä¸ªæ–‡æœ¬, è€—æ—¶ {duration:.2f}s")
            print(f"å‘é‡åŒ–é€Ÿåº¦: {throughput:.2f} texts/s")
            
        except Exception as e:
            duration = time.time() - start_time
            success = False
            throughput = 0
            print(f"å‘é‡åŒ–å¤±è´¥: {e}")
        
        self.resource_monitor.stop_monitoring()
        
        result = BenchmarkResult(
            test_name="vectorization",
            duration=duration,
            success=success,
            throughput=throughput,
            resource_usage=self.resource_monitor.get_current_metrics(),
            additional_metrics={
                'text_count': len(test_texts),
                'resource_summary': self.resource_monitor.get_resource_summary()
            }
        )
        
        self.benchmark_results.append(result)
        return result
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š ===")
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = {
            'platform': sys.platform,
            'python_version': sys.version,
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'gpu_info': self.resource_monitor.get_gpu_metrics()
        }
        
        # åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦
        benchmark_summary = {}
        for result in self.benchmark_results:
            benchmark_summary[result.test_name] = {
                'success': result.success,
                'duration': result.duration,
                'throughput': result.throughput,
                'latency_stats': result.latency_stats,
                'error_message': result.error_message
            }
        
        # æ€§èƒ½ç›‘æ§æ‘˜è¦
        performance_summary = self.performance_monitor.get_system_performance_summary()
        
        report = {
            'report_timestamp': datetime.now().isoformat(),
            'system_info': system_info,
            'benchmark_summary': benchmark_summary,
            'performance_monitoring': performance_summary,
            'detailed_results': [result.__dict__ for result in self.benchmark_results]
        }
        
        return report
    
    def save_report(self, filename: str = None) -> str:
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"hardware_performance_report_{timestamp}.json"
        
        report = self.generate_comprehensive_report()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"âœ“ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
            return filename
            
        except Exception as e:
            print(f"âš  ä¿å­˜æ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def print_summary_report(self):
        """æ‰“å°æ‘˜è¦æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ç¡¬ä»¶èµ„æºå ç”¨æŠ¥å‘Šæ‘˜è¦")
        print("="*80)
        
        # ç³»ç»Ÿä¿¡æ¯
        current_metrics = self.resource_monitor.get_current_metrics()
        print(f"\nğŸ“Š å½“å‰ç³»ç»ŸçŠ¶æ€:")
        print(f"  CPUä½¿ç”¨ç‡: {current_metrics.cpu_percent:.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨ç‡: {current_metrics.memory_percent:.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨é‡: {current_metrics.memory_used_gb:.2f}GB")
        print(f"  ç£ç›˜ä½¿ç”¨ç‡: {current_metrics.disk_usage_percent:.1f}%")
        
        if current_metrics.gpu_metrics:
            print(f"\nğŸ® GPUçŠ¶æ€:")
            for gpu in current_metrics.gpu_metrics:
                print(f"  GPU {gpu['id']} ({gpu['name']}):")
                if 'load' in gpu:
                    print(f"    è´Ÿè½½: {gpu['load']:.1f}%")
                if 'memory_percent' in gpu:
                    print(f"    æ˜¾å­˜: {gpu['memory_percent']:.1f}%")
        
        # åŸºå‡†æµ‹è¯•ç»“æœ
        if self.benchmark_results:
            print(f"\nğŸƒ åŸºå‡†æµ‹è¯•ç»“æœ:")
            for result in self.benchmark_results:
                status = "âœ“" if result.success else "âœ—"
                print(f"  {status} {result.test_name}:")
                print(f"    è€—æ—¶: {result.duration:.2f}s")
                if result.throughput:
                    print(f"    ååé‡: {result.throughput:.2f} ops/s")
                if result.latency_stats:
                    print(f"    å¹³å‡å»¶è¿Ÿ: {result.latency_stats['avg']:.3f}s")
                    print(f"    P95å»¶è¿Ÿ: {result.latency_stats['p95']:.3f}s")
        
        # æ€§èƒ½ç›‘æ§æ‘˜è¦
        perf_summary = self.performance_monitor.get_system_performance_summary()
        if 'total_operations' in perf_summary:
            print(f"\nğŸ“ˆ æ€§èƒ½ç›‘æ§æ‘˜è¦:")
            print(f"  æ€»æ“ä½œæ•°: {perf_summary['total_operations']}")
            print(f"  æˆåŠŸç‡: {perf_summary['overall_success_rate']:.1%}")
            print(f"  å¹³å‡è€—æ—¶: {perf_summary['avg_duration']:.2f}s")
            print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {perf_summary['avg_memory_usage']:.1f}MB")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ç¡¬ä»¶èµ„æºå ç”¨æŠ¥å‘Šç”Ÿæˆå™¨")
    print("="*50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = PerformanceBenchmark()
    
    # æ£€æŸ¥æµ‹è¯•æ–‡æ¡£
    test_documents = []
    upload_dir = Path("uploaded_documents")
    if upload_dir.exists():
        test_documents = [str(f) for f in upload_dir.glob("*.pdf")][:3]  # é™åˆ¶æµ‹è¯•æ–‡æ¡£æ•°é‡
    
    if not test_documents:
        print("è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£ï¼Œå°†è·³è¿‡æ–‡æ¡£å¤„ç†æµ‹è¯•")
    else:
        print(f"æ‰¾åˆ° {len(test_documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æœ‰å“ªäº›é‡è¦çš„æŠ€æœ¯ç‰¹æ€§ï¼Ÿ",
        "ç³»ç»Ÿçš„æ¶æ„æ˜¯æ€æ ·çš„ï¼Ÿ"
    ]
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºå‘é‡åŒ–æ€§èƒ½æµ‹è¯•ã€‚",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä¸ºå„è¡Œå„ä¸šå¸¦æ¥å˜é©ã€‚",
        "æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ä¹‹ä¸€ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"
    ] * 10  # æ‰©å±•æµ‹è¯•æ•°æ®
    
    try:
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        if test_documents:
            benchmark.run_document_processing_benchmark(test_documents)
        
        benchmark.run_vectorization_benchmark(test_texts)
        
        if test_documents:  # åªæœ‰åœ¨æœ‰æ–‡æ¡£çš„æƒ…å†µä¸‹æ‰è¿è¡ŒæŸ¥è¯¢æµ‹è¯•
            benchmark.run_query_latency_benchmark(test_queries, iterations=5)
        
        # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        report_file = benchmark.save_report()
        benchmark.print_summary_report()
        
        if report_file:
            print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        benchmark.resource_monitor.stop_monitoring()
        print("\næµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()