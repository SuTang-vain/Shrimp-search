"""
å¿«é€Ÿæµ‹è¯•å¢å¼ºRAGç³»ç»ŸV2çš„æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_model_selection():
    """æµ‹è¯•æ¨¡å‹é€‰æ‹©åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹é€‰æ‹©åŠŸèƒ½")
    print("="*50)
    
    try:
        from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2
        
        # åˆå§‹åŒ–LLMæ¥å£
        llm = EnhancedLLMInterfaceV2()
        
        # è·å–å¯ç”¨æ¨¡å‹
        available_models = llm.list_available_models()
        
        print("å¯ç”¨æ¨¡å‹:")
        for model_type, models in available_models.items():
            print(f"\n{model_type.upper()} æ¨¡å‹:")
            for key, info in models.items():
                status = "âœ… å¯ç”¨" if info['available'] else "âŒ ä¸å¯ç”¨"
                if model_type == 'local':
                    installed = "å·²å®‰è£…" if info.get('installed', False) else "æœªå®‰è£…"
                    status += f" ({installed})"
                print(f"  - {key}: {status}")
        
        # å°è¯•è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
        print("\nå°è¯•è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹...")
        selected_model = llm.auto_select_best_model()
        
        if selected_model:
            print(f"âœ… æˆåŠŸé€‰æ‹©æ¨¡å‹: {selected_model}")
            
            # æµ‹è¯•æ¨¡å‹
            test_result = llm.test_model()
            if test_result['success']:
                print(f"âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸ: {test_result['response'][:50]}...")
                print(f"   ç”Ÿæˆæ—¶é—´: {test_result['generation_time']:.2f}ç§’")
            else:
                print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {test_result['error']}")
        else:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*50)

def test_basic_generation():
    """æµ‹è¯•åŸºç¡€ç”ŸæˆåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€ç”ŸæˆåŠŸèƒ½")
    print("="*50)
    
    try:
        from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2
        
        llm = EnhancedLLMInterfaceV2()
        
        # è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
        if llm.auto_select_best_model():
            # æµ‹è¯•ç®€å•ç”Ÿæˆ
            test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
            print(f"æµ‹è¯•æç¤º: {test_prompt}")
            
            result = llm.generate(test_prompt, max_tokens=200, temperature=0.7)
            
            if not result.error:
                print(f"âœ… ç”ŸæˆæˆåŠŸ:")
                print(f"   æ¨¡å‹: {result.model_key} ({result.model_type})")
                print(f"   æ—¶é—´: {result.generation_time:.2f}ç§’")
                print(f"   å†…å®¹: {result.content[:200]}...")
            else:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {result.error}")
        else:
            print("âŒ æ— å¯ç”¨æ¨¡å‹")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*50)

def test_model_switching():
    """æµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½")
    print("="*50)
    
    try:
        from enhanced_llm_interface_v2 import EnhancedLLMInterfaceV2
        
        llm = EnhancedLLMInterfaceV2()
        available_models = llm.list_available_models()
        
        # æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹
        available_keys = []
        for model_type, models in available_models.items():
            for key, info in models.items():
                if info['available']:
                    if model_type == 'local' and info.get('installed', False):
                        available_keys.append(key)
                    elif model_type == 'remote':
                        available_keys.append(key)
        
        if len(available_keys) >= 1:
            # æµ‹è¯•åˆ‡æ¢åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            test_key = available_keys[0]
            print(f"å°è¯•åˆ‡æ¢åˆ°æ¨¡å‹: {test_key}")
            
            if llm.set_model(test_key):
                print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ°: {test_key}")
                
                current_model = llm.get_current_model()
                print(f"   å½“å‰æ¨¡å‹: {current_model['key']}")
                print(f"   æä¾›å•†: {current_model['provider']}")
                print(f"   ç±»å‹: {current_model['type']}")
                
                # ç®€å•æµ‹è¯•
                result = llm.generate("ä½ å¥½", max_tokens=50)
                if not result.error:
                    print(f"âœ… æµ‹è¯•ç”ŸæˆæˆåŠŸ: {result.content[:30]}...")
                else:
                    print(f"âŒ æµ‹è¯•ç”Ÿæˆå¤±è´¥: {result.error}")
            else:
                print(f"âŒ åˆ‡æ¢å¤±è´¥")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨æ¨¡å‹è¿›è¡Œåˆ‡æ¢æµ‹è¯•")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "="*50)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¢å¼ºRAGç³»ç»ŸV2 - å¿«é€ŸåŠŸèƒ½æµ‹è¯•")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    api_key = os.getenv('MODELSCOPE_API_KEY')
    print(f"ModelScope APIå¯†é’¥: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
    print()
    
    # è¿è¡Œæµ‹è¯•
    test_model_selection()
    test_basic_generation()
    test_model_switching()
    
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("="*60)

if __name__ == "__main__":
    main()