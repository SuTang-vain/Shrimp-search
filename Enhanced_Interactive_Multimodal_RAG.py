"""
å¢å¼ºçš„äº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ v2.0
è§£å†³äº†CAMELç¨³å®šæ€§ã€å¤šæ¨¡æ€æ”¯æŒã€ç½‘é¡µç ”ç©¶ã€æ€§èƒ½ç›‘æ§å’Œç•Œé¢ä½“éªŒç­‰é—®é¢˜
"""

import os
import sys
import json
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# å¯¼å…¥å¢å¼ºæ¨¡å—
from enhanced_llm_interface import EnhancedLLMInterface, LLMConfig
from enhanced_multimodal_processor import EnhancedMultimodalProcessor
from enhanced_web_research import EnhancedWebResearchSystem
from performance_monitor import PerformanceMonitor, PerformanceContext
from enhanced_user_interface import EnhancedUserInterface
from enhanced_document_manager import EnhancedDocumentManager, DocumentChunk

# åŸºç¡€ä¾èµ–
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# åµŒå…¥æ¨¡å‹
try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    print("âš  sentence-transformersåº“æœªå®‰è£…")
    EMBEDDING_AVAILABLE = False

class EnhancedVectorRetriever:
    """å¢å¼ºçš„å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, embedding_model, performance_monitor: Optional[PerformanceMonitor] = None):
        self.embedding_model = embedding_model
        self.performance_monitor = performance_monitor
        self.documents = []
        self.doc_metadata = []
        self.document_ids = []
        self.doc_embeddings = None
        print("âœ… å¢å¼ºå‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def add_documents(self, texts: List[str], metadata: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨"""
        if not texts:
            return
        
        operation_name = "add_documents"
        additional_data = {"document_count": len(texts)}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                self._add_documents_impl(texts, metadata)
        else:
            self._add_documents_impl(texts, metadata)
    
    def _add_documents_impl(self, texts: List[str], metadata: List[Dict[str, Any]]):
        """æ·»åŠ æ–‡æ¡£çš„å®ç°"""
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            new_ids = [f"doc_{len(self.documents) + i}" for i in range(len(texts))]
            
            # å­˜å‚¨æ–‡æ¡£
            self.documents.extend(texts)
            self.doc_metadata.extend(metadata)
            self.document_ids.extend(new_ids)
            
            # è®¡ç®—åµŒå…¥
            print(f"â³ æ­£åœ¨è®¡ç®— {len(texts)} ä¸ªæ–‡æ¡£çš„åµŒå…¥å‘é‡...")
            new_embeddings = self.embedding_model.encode(texts)
            
            if self.doc_embeddings is None:
                self.doc_embeddings = new_embeddings
            else:
                self.doc_embeddings = np.vstack([self.doc_embeddings, new_embeddings])
            
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(texts)} ä¸ªæ–‡æ¡£åˆ°æ£€ç´¢å™¨")
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def query(self, query_text: str, top_k: int = 5, filter_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢ç›¸å…³æ–‡æ¡£"""
        if not self.documents or self.doc_embeddings is None:
            print("âš  æ£€ç´¢å™¨ä¸­æ²¡æœ‰æ–‡æ¡£")
            return []
        
        operation_name = "vector_query"
        additional_data = {"query_length": len(query_text), "top_k": top_k, "filter_type": filter_type}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data) as ctx:
                return self._query_impl(query_text, top_k, filter_type)
        else:
            return self._query_impl(query_text, top_k, filter_type)
    
    def _query_impl(self, query_text: str, top_k: int, filter_type: Optional[str]) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢å®ç°"""
        try:
            # è®¡ç®—æŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query_text])
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
            
            # è·å–top_kç»“æœ
            top_indices = np.argsort(similarities)[::-1][:top_k * 2]
            
            results = []
            for idx in top_indices:
                if idx >= len(self.documents):
                    continue
                    
                doc_type = self.doc_metadata[idx].get('type', 'unknown')
                if filter_type and doc_type != filter_type:
                    continue
                
                results.append({
                    'text': self.documents[idx],
                    'similarity': float(similarities[idx]),
                    'metadata': self.doc_metadata[idx],
                    'id': self.document_ids[idx]
                })
                
                if len(results) >= top_k:
                    break
            
            print(f"âœ… æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            return []

class EnhancedRAGSystem:
    """å¢å¼ºçš„RAGç³»ç»Ÿ - é›†æˆæ‰€æœ‰æ”¹è¿›åŠŸèƒ½"""
    
    def __init__(self, api_key: str, config: Optional[LLMConfig] = None, 
                 enable_performance_monitoring: bool = True,
                 cache_dir: str = "document_cache"):
        if not api_key:
            raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor(enable_performance_monitoring) if enable_performance_monitoring else None
        
        try:
            print("ğŸš€ åˆå§‹åŒ–å¢å¼ºRAGç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–LLMæ¥å£
            self.llm = EnhancedLLMInterface(api_key=api_key, config=config)
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if not EMBEDDING_AVAILABLE:
                raise ImportError("sentence-transformersä¸å¯ç”¨")
            
            self.embedding_model = SentenceTransformer('intfloat/e5-large-v2')
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆå§‹åŒ–æ£€ç´¢å™¨
            self.vector_retriever = EnhancedVectorRetriever(
                self.embedding_model, 
                self.performance_monitor
            )
            
            # åˆå§‹åŒ–æ–‡æ¡£ç®¡ç†å™¨
            self.document_manager = EnhancedDocumentManager(
                cache_dir=cache_dir,
                enable_cache=True,
                max_cache_size_mb=1000
            )
            
            # åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨
            self.multimodal_processor = EnhancedMultimodalProcessor()
            
            # åˆå§‹åŒ–ç½‘é¡µç ”ç©¶ç³»ç»Ÿ
            self.web_research_system = EnhancedWebResearchSystem()
            
            # ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = False
            self.multimodal_content_store = {}
            self.current_sources = []  # æ”¯æŒå¤šä¸ªæ–‡æ¡£æº
            self.document_chunks_store = {}  # å­˜å‚¨æ–‡æ¡£å—
            
            print("âœ… å¢å¼ºRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def setup_knowledge_base(self, sources: List[str] = None, source_type: str = "path", 
                           force_reprocess: bool = False):
        """è®¾ç½®çŸ¥è¯†åº“ - æ”¯æŒå¤šæ–‡æ¡£å’Œå¢é‡æ›´æ–°"""
        if sources is None:
            sources = []
        
        operation_name = "setup_knowledge_base"
        additional_data = {"source_type": source_type, "source_count": len(sources)}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
        else:
            self._setup_knowledge_base_impl(sources, source_type, force_reprocess)
    
    def setup_single_document(self, source: str, source_type: str = "path", 
                            force_reprocess: bool = False):
        """è®¾ç½®å•ä¸ªæ–‡æ¡£ - å…¼å®¹åŸæœ‰æ¥å£"""
        self.setup_knowledge_base([source], source_type, force_reprocess)
    
    def _setup_knowledge_base_impl(self, sources: List[str], source_type: str, 
                                 force_reprocess: bool):
        """çŸ¥è¯†åº“è®¾ç½®å®ç° - æ”¯æŒå¤šæ–‡æ¡£å¤„ç†"""
        try:
            print(f"ğŸ“š æ­£åœ¨è®¾ç½®çŸ¥è¯†åº“...")
            print(f"æ–‡æ¡£æ•°é‡: {len(sources)}")
            print(f"æ¥æºç±»å‹: {source_type}")
            
            all_texts_to_embed = []
            all_metadata_to_embed = []
            total_chunks = 0
            processing_stats = {"text": 0, "image": 0, "table": 0, "other": 0}
            
            for source in sources:
                print(f"\nğŸ“„ å¤„ç†æ–‡æ¡£: {source}")
                
                try:
                    # ä½¿ç”¨æ–‡æ¡£ç®¡ç†å™¨å¤„ç†æ–‡æ¡£
                    if source_type == "path":
                        document_chunks = self.document_manager.process_document(
                            source, force_reprocess=force_reprocess
                        )
                    else:
                        # å¯¹äºURLï¼Œå…ˆä¸‹è½½å†å¤„ç†
                        document_chunks = self._process_url_document(source, force_reprocess)
                    
                    # å­˜å‚¨æ–‡æ¡£å—
                    self.document_chunks_store[source] = document_chunks
                    
                    # è½¬æ¢ä¸ºå‘é‡æ£€ç´¢æ ¼å¼
                    for chunk in document_chunks:
                        all_texts_to_embed.append(chunk.content)
                        
                        # å¢å¼ºå…ƒæ•°æ®
                        enhanced_metadata = chunk.metadata.copy()
                        enhanced_metadata.update({
                            'chunk_id': chunk.chunk_id,
                            'chunk_type': chunk.chunk_type,
                            'source_file': source
                        })
                        all_metadata_to_embed.append(enhanced_metadata)
                        
                        # ç»Ÿè®¡
                        chunk_type = chunk.chunk_type
                        if chunk_type in processing_stats:
                            processing_stats[chunk_type] += 1
                        else:
                            processing_stats["other"] += 1
                    
                    total_chunks += len(document_chunks)
                    print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {len(document_chunks)} ä¸ªå—")
                    
                except Exception as e:
                    print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
                    continue
            
            if not all_texts_to_embed:
                raise ValueError("æœªèƒ½ä»ä»»ä½•æ–‡æ¡£ä¸­æå–åˆ°å†…å®¹")
            
            # æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢å™¨
            print(f"\nâ³ æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
            self.vector_retriever.add_documents(all_texts_to_embed, all_metadata_to_embed)
            
            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            self.knowledge_base_initialized = True
            self.current_sources = sources
            
            print(f"\nâœ… çŸ¥è¯†åº“è®¾ç½®å®Œæˆ!")
            print(f"å¤„ç†æ–‡æ¡£æ•°: {len(sources)}")
            print(f"æ€»æ–‡æ¡£å—æ•°: {total_chunks}")
            print(f"å†…å®¹ç»Ÿè®¡: æ–‡æœ¬({processing_stats['text']}) å›¾åƒ({processing_stats['image']}) è¡¨æ ¼({processing_stats['table']}) å…¶ä»–({processing_stats['other']})")
            
            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            cache_stats = self.document_manager.get_cache_stats()
            print(f"ç¼“å­˜ç»Ÿè®¡: {cache_stats['cached_documents']} ä¸ªæ–‡æ¡£, {cache_stats['total_size_mb']:.1f}MB")
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _process_url_document(self, url: str, force_reprocess: bool) -> List[DocumentChunk]:
        """å¤„ç†URLæ–‡æ¡£"""
        import requests
        import tempfile
        from urllib.parse import urlparse
        
        try:
            # ä¸‹è½½æ–‡ä»¶
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            file_ext = os.path.splitext(parsed_url.path)[1]
            if not file_ext:
                file_ext = '.pdf'  # é»˜è®¤ä¸ºPDF
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=file_ext, delete=False) as temp_file:
                temp_file.write(response.content)
                temp_path = temp_file.name
            
            try:
                # å¤„ç†æ–‡æ¡£
                chunks = self.document_manager.process_document(temp_path, force_reprocess)
                
                # æ›´æ–°å…ƒæ•°æ®ä¸­çš„æºä¿¡æ¯
                for chunk in chunks:
                    chunk.metadata['original_url'] = url
                    chunk.metadata['source'] = url
                
                return chunks
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        except Exception as e:
            print(f"âŒ URLæ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return []
    
    def enhanced_query(self, query: str, retrieval_mode: str = "å¿«é€Ÿæ£€ç´¢") -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºæŸ¥è¯¢"""
        if not self.knowledge_base_initialized:
            raise RuntimeError("çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
        
        operation_name = f"enhanced_query_{retrieval_mode}"
        additional_data = {"query_length": len(query), "mode": retrieval_mode}
        
        if self.performance_monitor:
            with PerformanceContext(self.performance_monitor, operation_name, additional_data):
                return self._enhanced_query_impl(query, retrieval_mode)
        else:
            return self._enhanced_query_impl(query, retrieval_mode)
    
    def _enhanced_query_impl(self, query: str, retrieval_mode: str) -> Dict[str, Any]:
        """å¢å¼ºæŸ¥è¯¢å®ç°"""
        try:
            print(f"\nğŸ” æ‰§è¡Œ{retrieval_mode}: {query}")
            print("="*60)
            
            if retrieval_mode == "å¿«é€Ÿæ£€ç´¢":
                return self._quick_retrieval(query)
            elif retrieval_mode == "æ·±åº¦æ£€ç´¢":
                return self._deep_retrieval(query)
            elif retrieval_mode == "ä¸»é¢˜æ£€ç´¢":
                return self._topic_retrieval(query)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ£€ç´¢æ¨¡å¼: {retrieval_mode}")
                
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}"}
    
    def _quick_retrieval(self, query: str) -> Dict[str, Any]:
        """å¿«é€Ÿæ£€ç´¢ - åŸºç¡€å‘é‡æ£€ç´¢"""
        print("âš¡ æ‰§è¡Œå¿«é€Ÿæ£€ç´¢...")
        
        # ç›´æ¥å‘é‡æ£€ç´¢
        retrieved_docs = self.vector_retriever.query(query, top_k=3, filter_type='text')
        
        if not retrieved_docs:
            return {"error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"}
        
        # ç”Ÿæˆç­”æ¡ˆ
        context_texts = [doc['text'] for doc in retrieved_docs]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼š

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç®€æ´æ˜ç¡®çš„ç­”æ¡ˆï¼š"""
        
        final_answer = self.llm.generate(prompt, max_tokens=400)
        
        return {
            'original_query': query,
            'retrieved_docs': retrieved_docs,
            'final_answer': final_answer,
            'retrieval_method': 'å¿«é€Ÿæ£€ç´¢'
        }
    
    def _deep_retrieval(self, query: str) -> Dict[str, Any]:
        """æ·±åº¦æ£€ç´¢ - æŸ¥è¯¢é‡å†™+HyDE+RRF"""
        print("ğŸ”¬ æ‰§è¡Œæ·±åº¦æ£€ç´¢...")
        
        # æ­¥éª¤1: æŸ¥è¯¢é‡å†™
        print("ğŸ“ æ­¥éª¤1: æŸ¥è¯¢é‡å†™")
        rewritten_query = self._query_rewriting(query)
        
        # æ­¥éª¤2: HyDEç”Ÿæˆ
        print("ğŸ§  æ­¥éª¤2: HyDEå‡è®¾æ–‡æ¡£ç”Ÿæˆ")
        hyde_doc = self._hyde_generation(rewritten_query)
        
        # æ­¥éª¤3: å¤šè·¯æ£€ç´¢
        print("ğŸ” æ­¥éª¤3: å¤šè·¯æ£€ç´¢")
        all_results = []
        
        # åŸå§‹æŸ¥è¯¢æ£€ç´¢
        original_results = self.vector_retriever.query(query, top_k=5)
        all_results.append(original_results)
        
        # é‡å†™æŸ¥è¯¢æ£€ç´¢
        rewritten_results = self.vector_retriever.query(rewritten_query, top_k=5)
        all_results.append(rewritten_results)
        
        # HyDEæ–‡æ¡£æ£€ç´¢
        hyde_results = self.vector_retriever.query(hyde_doc, top_k=5)
        all_results.append(hyde_results)
        
        # å¤šæ¨¡æ€æ£€ç´¢
        image_results = self.vector_retriever.query(query, top_k=3, filter_type='image')
        if image_results:
            all_results.append(image_results)
        
        table_results = self.vector_retriever.query(query, top_k=3, filter_type='table')
        if table_results:
            all_results.append(table_results)
        
        # æ­¥éª¤4: RRFèåˆ
        print("ğŸ”€ æ­¥éª¤4: RRFç»“æœèåˆ")
        final_docs = self._rrf_fusion(all_results)
        
        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        print("ğŸ’¡ æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        context_texts = [doc['text'] for doc in final_docs[:5]]
        context_str = '\n---\n'.join(context_texts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”è¦æ±‚ï¼š
1. ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹
2. æä¾›å‡†ç¡®è¯¦ç»†çš„ç­”æ¡ˆ
3. å¼•ç”¨å…·ä½“æ–‡æ¡£å†…å®¹
4. ä½¿ç”¨æ¸…æ™°çš„ç»“æ„

è¯·æä¾›è¯¦ç»†ç­”æ¡ˆï¼š"""
        
        final_answer = self.llm.generate(prompt, max_tokens=800)
        
        return {
            'original_query': query,
            'rewritten_query': rewritten_query,
            'hyde_doc': hyde_doc,
            'retrieved_docs': final_docs,
            'final_answer': final_answer,
            'retrieval_method': 'æ·±åº¦æ£€ç´¢(é‡å†™+HyDE+RRF)'
        }
    
    def _topic_retrieval(self, query: str) -> Dict[str, Any]:
        """ä¸»é¢˜æ£€ç´¢ - PDF+ç½‘é¡µç»¼åˆåˆ†æ"""
        print("ğŸŒ æ‰§è¡Œä¸»é¢˜æ£€ç´¢...")
        
        # æ­¥éª¤1: PDFæ£€ç´¢
        print("ğŸ“„ æ­¥éª¤1: PDFæ–‡æ¡£æ£€ç´¢")
        pdf_docs = self.vector_retriever.query(query, top_k=5)
        
        # æ­¥éª¤2: ç½‘é¡µç ”ç©¶
        print("ğŸ” æ­¥éª¤2: ç½‘é¡µç ”ç©¶")
        web_research_result = self.web_research_system.research_topic(query, max_pages=3)
        
        # æ­¥éª¤3: ç»¼åˆåˆ†æ
        print("ğŸ§© æ­¥éª¤3: ç»¼åˆåˆ†æ")
        
        # åˆå¹¶å†…å®¹
        combined_context = ""
        
        # æ·»åŠ PDFå†…å®¹
        if pdf_docs:
            pdf_context = "\n".join([doc['text'] for doc in pdf_docs])
            combined_context += f"PDFæ–‡æ¡£å†…å®¹ï¼š\n{pdf_context}\n\n"
        
        # æ·»åŠ ç½‘é¡µå†…å®¹
        web_analysis = web_research_result.get('analysis', '')
        if web_analysis and not web_analysis.startswith("æœªèƒ½è·å–"):
            combined_context += f"ç½‘é¡µç ”ç©¶å†…å®¹ï¼š\n{web_analysis}\n\n"
        
        if not combined_context.strip():
            combined_context = f"å…³äº'{query}'çš„ä¿¡æ¯ä¸»è¦æ¥è‡ªPDFæ–‡æ¡£ï¼Œç½‘é¡µç ”ç©¶åŠŸèƒ½å½“å‰å—é™ã€‚"
        
        # ç”Ÿæˆç»¼åˆç­”æ¡ˆ
        prompt = f"""åŸºäºä»¥ä¸‹PDFæ–‡æ¡£å’Œç½‘é¡µç ”ç©¶ä¿¡æ¯ï¼Œå›ç­”é—®é¢˜ï¼š

{combined_context}

é—®é¢˜ï¼š{query}

è¯·æä¾›ç»¼åˆæ€§çš„è¯¦ç»†ç­”æ¡ˆï¼Œæ•´åˆPDFå’Œç½‘é¡µä¿¡æ¯ï¼š"""
        
        final_answer = self.llm.generate(prompt, max_tokens=800)
        
        return {
            'original_query': query,
            'pdf_docs': pdf_docs,
            'web_research': web_research_result,
            'final_answer': final_answer,
            'retrieval_method': 'ä¸»é¢˜æ£€ç´¢(PDF+ç½‘é¡µ)'
        }
    
    def _query_rewriting(self, query: str) -> str:
        """æŸ¥è¯¢é‡å†™"""
        try:
            prompt = f"""è¯·é‡å†™ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆæ–‡æ¡£æ£€ç´¢ï¼š

åŸå§‹æŸ¥è¯¢ï¼š{query}

é‡å†™è¦æ±‚ï¼š
1. ä¿æŒæ ¸å¿ƒæ„å›¾
2. ä½¿ç”¨æ›´å…·ä½“çš„è¯æ±‡
3. é€‚åˆå‘é‡æ£€ç´¢
4. åªè¿”å›é‡å†™åçš„æŸ¥è¯¢

é‡å†™æŸ¥è¯¢ï¼š"""
            
            rewritten = self.llm.generate(prompt, max_tokens=100)
            
            # æ¸…ç†ç»“æœ
            if rewritten and not rewritten.startswith("æŠ±æ­‰"):
                lines = rewritten.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('é‡å†™') and not line.startswith('åŸå§‹'):
                        return line
            
            return query
            
        except Exception as e:
            print(f"âš  æŸ¥è¯¢é‡å†™å¤±è´¥: {e}")
            return query
    
    def _hyde_generation(self, query: str) -> str:
        """HyDEå‡è®¾æ–‡æ¡£ç”Ÿæˆ"""
        try:
            prompt = f"""åŸºäºæŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§æ–‡æ¡£ç‰‡æ®µï¼š

æŸ¥è¯¢ï¼š{query}

è¦æ±‚ï¼š
1. ç”Ÿæˆ200-300å­—çš„æ–‡æ¡£ç‰‡æ®µ
2. ç›´æ¥å›ç­”æŸ¥è¯¢é—®é¢˜
3. ä½¿ç”¨ä¸“ä¸šå‡†ç¡®çš„è¯­è¨€
4. åŒ…å«ç›¸å…³æŠ€æœ¯ç»†èŠ‚

å‡è®¾æ–‡æ¡£ï¼š"""
            
            hyde_doc = self.llm.generate(prompt, max_tokens=300)
            
            if hyde_doc and not hyde_doc.startswith("æŠ±æ­‰"):
                return hyde_doc.strip()
            else:
                return query
                
        except Exception as e:
            print(f"âš  HyDEç”Ÿæˆå¤±è´¥: {e}")
            return query
    
    def _rrf_fusion(self, all_results: List[List[Dict]], k: int = 60) -> List[Dict[str, Any]]:
        """RRFèåˆå¤šä¸ªæ£€ç´¢ç»“æœ"""
        try:
            doc_scores = {}
            
            for results in all_results:
                for rank, doc in enumerate(results):
                    doc_id = doc.get('id', '')
                    if doc_id:
                        rrf_score = 1.0 / (k + rank + 1)
                        if doc_id in doc_scores:
                            doc_scores[doc_id]['score'] += rrf_score
                        else:
                            doc_scores[doc_id] = {
                                'doc': doc,
                                'score': rrf_score
                            }
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_docs = sorted(doc_scores.values(), key=lambda x: x['score'], reverse=True)
            
            # æ·»åŠ RRFåˆ†æ•°
            final_docs = []
            for item in sorted_docs:
                doc = item['doc'].copy()
                doc['rrf_score'] = item['score']
                final_docs.append(doc)
            
            return final_docs
            
        except Exception as e:
            print(f"âš  RRFèåˆå¤±è´¥: {e}")
            return all_results[0] if all_results else []
    
    def add_documents_to_knowledge_base(self, new_sources: List[str], 
                                      source_type: str = "path") -> bool:
        """å¢é‡æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        try:
            print(f"ğŸ“š å¢é‡æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“...")
            print(f"æ–°æ–‡æ¡£æ•°é‡: {len(new_sources)}")
            
            if not self.knowledge_base_initialized:
                print("âš ï¸ çŸ¥è¯†åº“æœªåˆå§‹åŒ–ï¼Œå°†è¿›è¡Œå®Œæ•´åˆå§‹åŒ–")
                self.setup_knowledge_base(new_sources, source_type)
                return True
            
            # è¿‡æ»¤å·²å­˜åœ¨çš„æ–‡æ¡£
            existing_sources = set(self.current_sources)
            truly_new_sources = [s for s in new_sources if s not in existing_sources]
            
            if not truly_new_sources:
                print("â„¹ï¸ æ‰€æœ‰æ–‡æ¡£éƒ½å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­")
                return True
            
            print(f"ğŸ“„ å®é™…æ–°å¢æ–‡æ¡£: {len(truly_new_sources)}")
            
            # å¤„ç†æ–°æ–‡æ¡£
            new_texts_to_embed = []
            new_metadata_to_embed = []
            
            for source in truly_new_sources:
                try:
                    if source_type == "path":
                        document_chunks = self.document_manager.process_document(source)
                    else:
                        document_chunks = self._process_url_document(source, False)
                    
                    self.document_chunks_store[source] = document_chunks
                    
                    for chunk in document_chunks:
                        new_texts_to_embed.append(chunk.content)
                        enhanced_metadata = chunk.metadata.copy()
                        enhanced_metadata.update({
                            'chunk_id': chunk.chunk_id,
                            'chunk_type': chunk.chunk_type,
                            'source_file': source
                        })
                        new_metadata_to_embed.append(enhanced_metadata)
                    
                    print(f"âœ… æ–°å¢æ–‡æ¡£å¤„ç†å®Œæˆ: {source} ({len(document_chunks)} ä¸ªå—)")
                    
                except Exception as e:
                    print(f"âŒ æ–°å¢æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
                    continue
            
            if new_texts_to_embed:
                # æ·»åŠ åˆ°å‘é‡æ£€ç´¢å™¨
                self.vector_retriever.add_documents(new_texts_to_embed, new_metadata_to_embed)
                
                # æ›´æ–°æºåˆ—è¡¨
                self.current_sources.extend(truly_new_sources)
                
                print(f"âœ… å¢é‡æ›´æ–°å®Œæˆ! æ–°å¢ {len(new_texts_to_embed)} ä¸ªæ–‡æ¡£å—")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–°æ–‡æ¡£")
                return False
                
        except Exception as e:
            print(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def remove_documents_from_knowledge_base(self, sources_to_remove: List[str]) -> bool:
        """ä»çŸ¥è¯†åº“ä¸­ç§»é™¤æ–‡æ¡£"""
        try:
            print(f"ğŸ—‘ï¸ ä»çŸ¥è¯†åº“ä¸­ç§»é™¤æ–‡æ¡£...")
            print(f"å¾…ç§»é™¤æ–‡æ¡£: {len(sources_to_remove)}")
            
            removed_count = 0
            for source in sources_to_remove:
                if source in self.current_sources:
                    # ä»å­˜å‚¨ä¸­ç§»é™¤
                    if source in self.document_chunks_store:
                        del self.document_chunks_store[source]
                    
                    # ä»æºåˆ—è¡¨ä¸­ç§»é™¤
                    self.current_sources.remove(source)
                    
                    # æ¸…ç†ç¼“å­˜
                    self.document_manager.clear_cache(source)
                    
                    removed_count += 1
                    print(f"âœ… å·²ç§»é™¤: {source}")
                else:
                    print(f"âš ï¸ æ–‡æ¡£ä¸å­˜åœ¨: {source}")
            
            if removed_count > 0:
                print(f"âœ… æˆåŠŸç§»é™¤ {removed_count} ä¸ªæ–‡æ¡£")
                
                # å¦‚æœç§»é™¤äº†æ–‡æ¡£ï¼Œéœ€è¦é‡å»ºå‘é‡ç´¢å¼•
                if self.current_sources:
                    print("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•...")
                    self._rebuild_vector_index()
                else:
                    print("â„¹ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º")
                    self.knowledge_base_initialized = False
                
                return True
            else:
                print("â„¹ï¸ æ²¡æœ‰ç§»é™¤ä»»ä½•æ–‡æ¡£")
                return False
                
        except Exception as e:
            print(f"âŒ ç§»é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def _rebuild_vector_index(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        try:
            # é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨
            self.vector_retriever = EnhancedVectorRetriever(
                self.embedding_model, 
                self.performance_monitor
            )
            
            # é‡æ–°æ·»åŠ æ‰€æœ‰æ–‡æ¡£
            all_texts = []
            all_metadata = []
            
            for source in self.current_sources:
                if source in self.document_chunks_store:
                    chunks = self.document_chunks_store[source]
                    for chunk in chunks:
                        all_texts.append(chunk.content)
                        enhanced_metadata = chunk.metadata.copy()
                        enhanced_metadata.update({
                            'chunk_id': chunk.chunk_id,
                            'chunk_type': chunk.chunk_type,
                            'source_file': source
                        })
                        all_metadata.append(enhanced_metadata)
            
            if all_texts:
                self.vector_retriever.add_documents(all_texts, all_metadata)
                print(f"âœ… å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ: {len(all_texts)} ä¸ªæ–‡æ¡£å—")
            
        except Exception as e:
            print(f"âŒ é‡å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        cache_stats = self.document_manager.get_cache_stats()
        
        return {
            "LLMæ¥å£": self.llm.get_status(),
            "åµŒå…¥æ¨¡å‹": {"available": EMBEDDING_AVAILABLE, "details": "intfloat/e5-large-v2"},
            "æ–‡æ¡£ç®¡ç†": {
                "æ”¯æŒæ ¼å¼": list(self.document_manager.supported_formats.keys()),
                "ç¼“å­˜æ–‡æ¡£æ•°": cache_stats['cached_documents'],
                "ç¼“å­˜å¤§å°": f"{cache_stats['total_size_mb']:.1f}MB",
                "ç¼“å­˜ä½¿ç”¨ç‡": f"{cache_stats['cache_usage_percent']:.1f}%"
            },
            "å¤šæ¨¡æ€å¤„ç†": {
                "available": True,
                "details": f"OCR: {self.multimodal_processor.image_ocr_available}, è¡¨æ ¼: {self.multimodal_processor.table_processing_available}"
            },
            "ç½‘é¡µç ”ç©¶": self.web_research_system.get_research_statistics(),
            "çŸ¥è¯†åº“": {
                "initialized": self.knowledge_base_initialized, 
                "æ–‡æ¡£æ•°é‡": len(self.current_sources),
                "æ–‡æ¡£åˆ—è¡¨": self.current_sources[:5] + ["..."] if len(self.current_sources) > 5 else self.current_sources
            },
            "æ€§èƒ½ç›‘æ§": {"enabled": self.performance_monitor is not None}
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if self.performance_monitor:
            return self.performance_monitor.get_system_performance_summary()
        else:
            return {"message": "æ€§èƒ½ç›‘æ§æœªå¯ç”¨"}

class EnhancedInteractiveMultimodalRAG:
    """å¢å¼ºçš„äº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self):
        self.api_key = None
        self.rag_system = None
        self.ui = EnhancedUserInterface()
        self.system_initialized = False
    
    def initialize_system(self) -> bool:
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        try:
            print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¢å¼ºRAGç³»ç»Ÿ...")
            
            # åŠ è½½ç¯å¢ƒå˜é‡
            load_dotenv()
            
            # è·å–APIå¯†é’¥
            self.api_key = os.getenv('MODELSCOPE_SDK_TOKEN')
            if not self.api_key:
                self.ui.display_error("è¯·è®¾ç½®MODELSCOPE_SDK_TOKENç¯å¢ƒå˜é‡")
                return False
            
            # åˆ›å»ºLLMé…ç½®
            config = LLMConfig(
                model_name="Qwen/Qwen2.5-72B-Instruct",
                max_tokens=1000,
                temperature=0.7,
                timeout=60,
                max_retries=3
            )
            
            # åˆå§‹åŒ–RAGç³»ç»Ÿ
            self.rag_system = EnhancedRAGSystem(
                api_key=self.api_key,
                config=config,
                enable_performance_monitoring=True
            )
            
            self.system_initialized = True
            self.ui.display_success("å¢å¼ºRAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ!")
            
            # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
            status = self.rag_system.get_system_status()
            self.ui.display_system_status(status)
            
            return True
            
        except Exception as e:
            self.ui.display_error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def setup_knowledge_base(self) -> bool:
        """è®¾ç½®çŸ¥è¯†åº“ - æ”¯æŒå¤šæ–‡æ¡£"""
        try:
            # è·å–æ–‡æ¡£æ¥æº
            sources, source_type = self.ui.get_document_sources()
            
            if not sources:
                self.ui.display_error("æ–‡æ¡£æ¥æºä¸èƒ½ä¸ºç©º")
                return False
            
            # éªŒè¯è¾“å…¥
            if source_type == "path":
                for source in sources:
                    if not os.path.exists(source):
                        self.ui.display_error(f"æ–‡ä»¶ä¸å­˜åœ¨: {source}")
                        return False
            
            # è®¾ç½®çŸ¥è¯†åº“
            self.ui.display_loading(f"æ­£åœ¨åŠ è½½ {len(sources)} ä¸ªæ–‡æ¡£...")
            self.rag_system.setup_knowledge_base(sources, source_type)
            
            self.ui.display_success("çŸ¥è¯†åº“è®¾ç½®å®Œæˆ!")
            return True
            
        except Exception as e:
            self.ui.display_error(f"çŸ¥è¯†åº“è®¾ç½®å¤±è´¥: {e}")
            return False
    
    def manage_documents(self):
        """æ–‡æ¡£ç®¡ç†åŠŸèƒ½"""
        while True:
            try:
                action = self.ui.get_document_management_action()
                
                if action == "add":
                    # æ·»åŠ æ–‡æ¡£
                    sources, source_type = self.ui.get_document_sources()
                    if sources:
                        self.ui.display_loading(f"æ­£åœ¨æ·»åŠ  {len(sources)} ä¸ªæ–‡æ¡£...")
                        success = self.rag_system.add_documents_to_knowledge_base(sources, source_type)
                        if success:
                            self.ui.display_success("æ–‡æ¡£æ·»åŠ æˆåŠŸ!")
                        else:
                            self.ui.display_error("æ–‡æ¡£æ·»åŠ å¤±è´¥!")
                
                elif action == "remove":
                    # ç§»é™¤æ–‡æ¡£
                    current_sources = self.rag_system.current_sources
                    if not current_sources:
                        self.ui.display_warning("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                        continue
                    
                    sources_to_remove = self.ui.select_documents_to_remove(current_sources)
                    if sources_to_remove:
                        self.ui.display_loading(f"æ­£åœ¨ç§»é™¤ {len(sources_to_remove)} ä¸ªæ–‡æ¡£...")
                        success = self.rag_system.remove_documents_from_knowledge_base(sources_to_remove)
                        if success:
                            self.ui.display_success("æ–‡æ¡£ç§»é™¤æˆåŠŸ!")
                        else:
                            self.ui.display_error("æ–‡æ¡£ç§»é™¤å¤±è´¥!")
                
                elif action == "list":
                    # åˆ—å‡ºæ–‡æ¡£
                    current_sources = self.rag_system.current_sources
                    if current_sources:
                        self.ui.display_document_list(current_sources)
                    else:
                        self.ui.display_warning("çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£")
                
                elif action == "cache":
                    # ç¼“å­˜ç®¡ç†
                    cache_stats = self.rag_system.document_manager.get_cache_stats()
                    self.ui.display_cache_stats(cache_stats)
                    
                    if self.ui.ask_clear_cache():
                        self.rag_system.document_manager.clear_cache()
                        self.ui.display_success("ç¼“å­˜å·²æ¸…ç†!")
                
                elif action == "back":
                    break
                    
            except Exception as e:
                self.ui.display_error(f"æ–‡æ¡£ç®¡ç†æ“ä½œå¤±è´¥: {e}")
    
    def run_interactive_session(self):
        """è¿è¡Œäº¤äº’å¼ä¼šè¯"""
        # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
        self.ui.print_welcome()
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        if not self.initialize_system():
            return
        
        # è®¾ç½®çŸ¥è¯†åº“
        if not self.setup_knowledge_base():
            return
        
        # ä¸»æŸ¥è¯¢å¾ªç¯
        while True:
            try:
                # è·å–ç”¨æˆ·æŸ¥è¯¢
                query = self.ui.get_user_query()
                
                if not query:
                    continue
                elif query.lower() in ['quit', 'exit', 'é€€å‡º']:
                    break
                elif query.lower() in ['status', 'çŠ¶æ€']:
                    status = self.rag_system.get_system_status()
                    self.ui.display_system_status(status)
                    continue
                elif query.lower() in ['performance', 'æ€§èƒ½']:
                    perf_data = self.rag_system.get_performance_summary()
                    self.ui.display_performance_summary(perf_data)
                    continue
                elif query.lower() in ['reload', 'é‡æ–°åŠ è½½']:
                    if self.setup_knowledge_base():
                        self.ui.display_success("çŸ¥è¯†åº“å·²é‡æ–°åŠ è½½")
                    continue
                elif query.lower() in ['manage', 'docs', 'æ–‡æ¡£ç®¡ç†']:
                    self.manage_documents()
                    continue
                
                # è·å–æ£€ç´¢æ¨¡å¼
                retrieval_mode = self.ui.get_retrieval_mode()
                
                # æ‰§è¡ŒæŸ¥è¯¢
                self.ui.display_loading(f"æ­£åœ¨æ‰§è¡Œ{retrieval_mode}...")
                results = self.rag_system.enhanced_query(query, retrieval_mode)
                
                # æ˜¾ç¤ºç»“æœ
                self.ui.display_results(results)
                
                # è¯¢é—®æ˜¯å¦ä¿å­˜ç»“æœ
                if 'error' not in results and self.ui.ask_save_results():
                    self.ui.save_results_to_file(results)
                
                # è¯¢é—®æ˜¯å¦ç»§ç»­
                if not self.ui.ask_continue():
                    break
                    
            except KeyboardInterrupt:
                self.ui.display_warning("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
                if not self.ui.ask_continue():
                    break
            except Exception as e:
                self.ui.display_error(f"ç³»ç»Ÿé”™è¯¯: {e}")
                continue
        
        # æ˜¾ç¤ºæœ€ç»ˆæ€§èƒ½æŠ¥å‘Š
        if self.rag_system and self.rag_system.performance_monitor:
            print("\n" + "="*60)
            print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
            print("="*60)
            self.rag_system.performance_monitor.print_performance_report()
            
            # è¯¢é—®æ˜¯å¦ä¿å­˜æ€§èƒ½æ•°æ®
            if self.ui.ask_save_results():
                self.rag_system.performance_monitor.save_metrics_to_file()
        
        self.ui.display_success("æ„Ÿè°¢ä½¿ç”¨å¢å¼ºçš„äº¤äº’å¼å¤šæ¨¡æ€RAGç³»ç»Ÿ!")

def main():
    """ä¸»å‡½æ•°"""
    try:
        enhanced_rag = EnhancedInteractiveMultimodalRAG()
        enhanced_rag.run_interactive_session()
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
